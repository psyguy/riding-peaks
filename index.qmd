---
title: "Level-1 and level-2 estimates of the multilevel cosinor model"
author: "MH Manuel Haqiqatkhah"
date: 2024-12-10
date-modified: last-modified
date-format: "YYYY-MM-DD"
format: html
execute:
  eval: false
  echo: true
editor: visual
bibliography: references.bib
---

# Intro

In this repository, I will fit explore multilevel modeling of the cosinor model, starting with single-component cosinor model fitted to positive affect (PA) ESM time series data from Leuven Wave 1 of @erbas_2018_WhyDonAlways, and heart rate (fitbit) data from @yfantidou_2022_LifeSnaps4monthMultimodal. The processed datasets (saved as `d_leuven_joined.rds` and `d_fitbit_joined.rds` in the `data` folder) contains ILD measured values in `y`, the base-10 clock hour (between 0 and 24) in `t`, the regressors `co` ($\cos(\frac{2\pi}{24}t)$) and `si` ($\sin(\frac{2\pi}{24}t)$) calculated for each `t`, baseline PA (`b_pa`), and baseline NA (`bl_na`).

The required packages are as follows:

```{r}

library(brms)
library(psych)
library(Rfast)
library(MASS)
library(tidyverse)
library(patchwork)
library(ggthemes)
library(viridis)
library(latex2exp)
library(here)
library(ggh4x)
library(rlang)
library(data.table)
library(circular)

```

# Analyses

We load the datasets, and run the analyses that follow for each separately.

```{r}
#| label: read-data

# For the Leuven dataset; choose pa
item_ <- "pa"
d <- readRDS(here("data",
                  "d_leuven_joined.rds")) %>%
  filter(item == item_)

# For fitbit dataset
item_ <- "fitbit"
d <- readRDS(here("data",
                  "d_fitbit_joined.rds"))


# Get the baseline PA and NA for the selected dataset
d_bl <- d %>%
  select(id, bl_pa, bl_na) %>% 
  distinct()

```

## Fitting the cosinor model

We fit two versions of the cosinor model.

### Model with fixed residual variance

One with residual variance estimated as fixed effects (i.e., everyone getting the same $\sigma^2$), that is,

$$
\begin{aligned}
&M_i = M + m_i \\
&C_i = C + c_i \\
&S_i = S + c_i \\
&\sigma_i = \sigma
\end{aligned} \quad,
$$

where

$$
\begin{bmatrix}M_i \\C_i \\S_i \end{bmatrix}
\sim\mathcal{MN}(
\begin{bmatrix}M \\C \\S \end{bmatrix}
,\mathbf{\Sigma})
$$

using the following `brms` code:

```{r}
#| label: brms-fixed-residual-variance

(st <- Sys.time())
m_fixed_var <- brm(
  y ~ 1 + co + si + (1 + co + si | id),
  data = d,
  backend = "cmdstanr",
  chains = 4,
  iter = 3000,
  threads = threading(2),
  cores = 8)
Sys.time() - st
beepr::beep(1)

saveRDS(m_fixed_var,
        here("fits",
            paste0("brms_",
                   item_,
                   "_fixed_var.rds")
            )
        )
```

### Model with random residual variance

We also fit a model with random residual variance, in which we also include an equation for the log of the individual specific variances $\rho_i = \log\sigma_i$ and the log of the fixed variance are $\rho = \log\sigma$, leading to

$$
\begin{aligned}
&M_i = M + m_i \\
&C_i = C + c_i \\
&S_i = S + c_i \\
&\rho_i = \rho + \varrho_i
\end{aligned} \quad ,
$$

with

$$
\begin{bmatrix}M_i \\C_i \\S_i \\\rho_i\end{bmatrix}
\sim\mathcal{MN}(
\begin{bmatrix}M \\C \\S \\\rho\end{bmatrix}
,\mathbf{\Sigma})
$$

using the following `brms` code:

```{r}
#| label: brms-random-residual-variance

(st <- Sys.time())
m_random_var <- brm(
  brmsformula(
    y ~ 1 + co + si + (1 + co + si | i | id),
    sigma ~ 1 + (1 | i | id)),
  data = d,
  backend = "cmdstanr",
  chains = 4,
  iter = 3000,
  threads = threading(2),
  cores = 8)
Sys.time() - st
beepr::beep(3)

saveRDS(m_random_var,
        here("fits",
             paste0("brms_",
                    item_,
                    "_random_var.rds")
             )
        )
```

Note that `| i | id` in the random effect specification sets the grouping variable `id` and assures to estimate the correlation between the random intercept and random slope in the first equation (for `y`) and the random intercept in the second equation (for `sigma`). We move on doing the analyses with the second model, so,

```{r}

# Model to move forward with
m_fit <- readRDS(here("fits",
             paste0("brms_",
                    item_,
                    "_random_var.rds")))
```

### Model outputs

The model `m_fit` contains samples from $D=6,000$ HMC draws, that include $4 \times D$ values for the fixed effects $M^{(d)}$, $C^{(d)}$, $S^{(d)}$, and $\rho^{(d)}$ (reported, respectively, as `b_Intercept`, `b_co`, `b_si`, and `b_sigma_Intercept`), $(6+4) \times D$ values for the correlations and SDs of $\mathbf{\Sigma}^{(d)}$, and $4 \times N \times D$ person-specific the random effects $m_i^{(d)}$, $c_i^{(d)}$, $s_i^{(d)}$, and $\varrho_i^{(d)}$ (reported, respectively, as `Intercept`, `co`, `si`, and `sigma_Intercept`).

From these, at each draw $d$, we calculate the person-specific parameters $M_i^{(d)}$, $C_i^{(d)}$, $S_i^{(d)}$, and $\rho_i^{(d)}$ by summing the fixed and random effects and storing them, respectively, in `mesor`, `co`, `si` and `logsigma` columns. Based on these, we calculate person-specific SD (`sigma = exp(logsigma)`) and variance (`sigma2 = exp(logsigma)^2`), and the implied amplitude `amp` and phase (correctly calculated with `atan2(si,co)` in `phi`, and incorrectly calculate using `atan(si/co)` in `phi_atan`), and do these transformations for fixed effects (the `b_` variables). We also calculate the phase assuming starting the cycle, instead of midnight, at 6:00 (`phi.begins6`) and at 12:00 (`phi.begins12`).

```{r}
#| label: extract-samples

# Taking out the individual-specific sampled parameter estimates
d_draws <- as_draws_df(m_fit) %>%
mutate(iteration = row_number()) %>%
  pivot_longer(
    cols = starts_with("r_id"),
    names_pattern = "r_id(_{0,2}sigma)?\\[(\\d+),(.*)\\]",
    names_to = c("sigma_part", "id", "term"),
    values_to = "value"
  ) %>%
  mutate(term = if_else(sigma_part == "__sigma", paste0("sigma_", term), term)) %>%
  pivot_wider(
    id_cols = c(iteration, id, b_Intercept, b_co, b_si, b_sigma_Intercept),
    names_from = term,
    values_from = value
  ) %>%
  mutate(
    mesor = Intercept + b_Intercept,
    co = co + b_co,
    si = si + b_si,
    logsigma = sigma_Intercept + b_sigma_Intercept
  ) %>%
  select(iteration, id,
         mesor, co, si, logsigma,
         b_Intercept, b_co, b_si, b_sigma_Intercept) %>%
  rename(b_logsigma = b_sigma_Intercept) %>%
  mutate(
    sigma = exp(logsigma),
    sigma2 = exp(logsigma)^2,
    amp = sqrt(si^2 + co^2),
    phi = atan2(si, co) %% (2*pi),
    phi_atan = atan(si/co) %% (2*pi),
    b_amp = sqrt(b_si^2 + b_co^2),
    b_phi = atan2(b_si, b_co) %% (2*pi),
    b_phi_atan = atan(b_si/b_co) %% (2*pi)
  ) %>%
  mutate(phi.begins6 = (phi - pi/2) %% (2*pi),
         phi.begins12 = (phi - pi) %% (2*pi)) %>% 
  mutate(id = id %>% as.integer())

d_draws %>%
  cbind(item = item_, .) %>%
  saveRDS(
    here("fits",
         paste0("draws_", item_, "_random_var.rds"))
    )

beepr::beep(5)
```

## Obtaining estimates

### Level-1 estimates

To get person-specific summary statistics, we do calculations across iterations (i.e., only `group_by(id) %>% summarize(...)`) to get mean, median, and 95%CI width of our estimated linear parameters, and for the circular parameters (`phi`, `phi.begins6`, `phi.begins12`), once incorrectly (using linear methods) and once correctly (using circular methods).

We also need to get the complementary of posterior predictive p-value using the percentage of density in which $[0,0]$ falls by finding the $\alpha\%\text{-HDR}$ (highest density region) for `amp` based on samples of `co` and `si`, which we do with `compute_alpha_hdr`:

```{r}
#| label: compute_alpha_hdr

# Load required packages
require(ks)
require(ggplot2)
require(metR)

compute_alpha_hdr <-
  function(d,
           x_0 = 0,
           y_0 = 0,
           n_grid = 200){
    # 1. Kernel density estimation
    fhat <- kde(d, gridsize = c(n_grid, n_grid))
    
    xseq <- fhat$eval.points[[1]]
    yseq <- fhat$eval.points[[2]]
    zmat <- fhat$estimate
    
    dx <- diff(xseq)[1]
    dy <- diff(yseq)[1]
    cell_area <- dx * dy
    
    # 2. Compute cumulative probability distribution to determine HDR thresholds
    dens_vec <- as.vector(zmat)
    dens_sorted <- sort(dens_vec, decreasing = TRUE)
    cumulative_mass <- cumsum(dens_sorted * cell_area)
    cumulative_mass <-
      cumulative_mass / max(cumulative_mass)  # normalize
    
    # 3. Evaluate density at the given point (x_0, y_0)
    f_point <- predict(fhat, x = cbind(x_0, y_0))
    
    # Find where f_point fits in the density distribution
    # Identify the position of f_point in the sorted densities
    idx <- which(dens_sorted < f_point)[1]
    
    if (is.na(idx)) {
      # f_point is >= all densities, alpha_hdr ~ 1
      alpha_hdr <- 1
    } else {
      # f_point lies between dens_sorted[idx-1] and dens_sorted[idx]
      # if idx > 1, cumulative_mass[idx-1] gives us a close approximation
      if (idx > 1) {
        alpha_hdr <- cumulative_mass[idx - 1]
      } else {
        # idx = 1 means f_point < dens_sorted[1], so alpha is slightly less than cumulative_mass[1]
        alpha_hdr <- cumulative_mass[1]
      }
    }
    
    return(round(alpha_hdr * 100, 1))
  }
```

We then use this function along other ones

```{r}
#| label: extract-level1-estimates

# Helper function for linear stats
compute_linear_stats <- function(data, variable) {
  tibble(
    mean = mean(data[[variable]], na.rm = TRUE),
    median = median(data[[variable]], na.rm = TRUE),
    ci_width = quantile(data[[variable]], 0.975, na.rm = TRUE) - quantile(data[[variable]], 0.025, na.rm = TRUE)
  ) %>%
    rename_with(~ paste0(variable, "_", .))
}

# Helper function for circular stats
compute_circular_stats <- function(data, variable) {
  tibble(
    circ_mean = (mean.circular(data[[variable]]) %% (2 * pi)) %>% as.numeric(),
    circ_median = (median.circular(data[[variable]]) %% (2 * pi)) %>% as.numeric(),
    circ_ci_width = (
      (quantile.circular(data[[variable]], 0.975) - quantile.circular(data[[variable]], 0.025)) %% (2 * pi)
    ) %>% as.numeric()
  ) %>%
    rename_with(~ paste0(variable, "_", .))
}

# Apply calculations for all variables
ests_level1 <- d_draws %>%
  # Change this to group_by(iteration) for level1.5
  group_by(id) %>%
  summarise(
    bind_cols(
      # Linear calculations
      compute_linear_stats(cur_data(), "mesor"),
      compute_linear_stats(cur_data(), "si"),
      compute_linear_stats(cur_data(), "co"),
      compute_linear_stats(cur_data(), "amp"),
      compute_linear_stats(cur_data(), "logsigma"),
      compute_linear_stats(cur_data(), "sigma"),
      compute_linear_stats(cur_data(), "sigma2"),
      compute_linear_stats(cur_data(), "phi"),
      compute_linear_stats(cur_data(), "phi.begins6"),
      compute_linear_stats(cur_data(), "phi.begins12"),
      # Circular calculations
      compute_circular_stats(cur_data(), "phi"),
      compute_circular_stats(cur_data(), "phi.begins6"),
      compute_circular_stats(cur_data(), "phi.begins12")
    ),
    # Compute amp_hdr using compute_alpha_hdr
    amp_hdr = compute_alpha_hdr(cbind(cur_data()$co, cur_data()$si))
  ) %>%
  relocate(amp_hdr, .before = logsigma_mean)

ests_level1 %>%
  full_join(d_bl, by = "id") %>%
  cbind(item = item_, .) %>%
  saveRDS(
    here("fits",
         paste0("ests_level1_", item_, "_random_var.rds"))
    )

beepr::beep(10)

```

### Level-2 estimates

Importantly, we want to estimate linear and correlations between MESOR, amplitude, and residual variance across individuals for each sample.

To do so, we need to first define a function to calculate Mardia's circular-linear rank correlation:

```{r}

# Mardia circular-linear rank correlation
cor_mardia <- function(theta, x) {
  # Remove NAs
  valid <- complete.cases(theta, x)
  theta <- theta[valid]
  x <- x[valid]
  n <- length(theta)
  if (n == 0) stop("No valid data after removing NAs.")
  
  # Rank and compute theta star
  r_theta <- rank(theta, ties.method = "average")
  r_x <- rank(x, ties.method = "average")
  r_theta_star <- r_theta * 2 * pi / n
  
  # Precompute sine and cosine
  cos_theta_star <- cos(r_theta_star)
  sin_theta_star <- sin(r_theta_star)
  
  # Compute T_c and T_s
  T_c <- sum(r_x * cos_theta_star)
  T_s <- sum(r_x * sin_theta_star)
  
  # Calculate coefficient 'a'
  a <- ifelse(
    n %% 2 == 0,
    1 / (1 + 5 / tan(pi / n)^2 + 4 / tan(pi / n)^4),
    2 * sin(pi / n)^4 / (1 + cos(pi / n))^3
  )
  
  # Final D value (rank correlation, values between 0 and 1)
  D <- a * (T_c^2 + T_s^2)
  
  # Calculate U_n and p-value
  U_n <- 24 * (T_c^2 + T_s^2) / (n^2 * (n + 1))
  p.value <- 1 - pchisq(U_n, df = 2)
  
  # Return results as a list
  return(list(estimate = D, p.value = p.value, U_n = U_n))
}
```

To make the code cleaner, we use a helper function that, for correlations where $\phi$ is involved, calculates various correlations (Pearson's, Spearman's rank, Kendall's rank, as well as Johnson-Wehrly-Mardia, and Mardia's rank) for us:

```{r}

# Create correlation function with linear and circular methods
compute_correlations <- function(x, y) {
  
  # Removing NA pairs
  z <- cbind(x, y) %>% na.omit()
  x <- z[,1]
  y <- z[,2]
  
  ## x is a circular variable, y is a linear variable
  cor_mardia <- cor_mardia(x, y)
  
  tibble(
    ## Linear correlations
    # Pearson's
    pearson_cor = cor(x, y),
    pearson_pval = cor.test(x, y)$p.value,
    # Spearman's
    spearman_cor = cor(x, y, method = "spearman"),
    spearman_pval = cor.test(x, y, method = "spearman")$p.value,# Kendall's tau
    kendall_cor = cor(x, y, method = "kendall"),
    kendall_pval = cor.test(x, y, method = "kendall")$p.value,
    ## Circular correlations
    # Johnson-Wehrly-Mardia correlation (sinusoidal assumption)
    jwm_cor = tryCatch(
      circlin.cor(x, y)[1],
      error = function(e)
        NA_real_
    ),
    jwm_pval = tryCatch(
      circlin.cor(x, y)[2],
      error = function(e)
        NA_real_
    ),
    # Mardia's rank correlation (no assumption)
    mard_cor = cor_mardia$estimate,
    mard_pval = cor_mardia$p.value
  )
}
```

We then use the functions to do the calculations, make a long dataframe, and save it as an `.RDS` file:

```{r}
#| label: extract-level2-estimates

# Add baseline bl_pa and bl_na to d_draws if not added yet
if(sum(c("bl_pa", "bl_na") %in% names(d_draws)) == FALSE)
  d_draws <- d_draws %>%
    full_join(d_bl,
              by = "id")

# Compute correlations for all combinations
cors_lin_angle <- d_draws %>%
  group_by(iteration) %>%
  summarise(
    crossing(
      linear_variable = c("mesor",
                          "logsigma",
                          "sigma",
                          "sigma2",
                          "bl_pa",
                          "bl_na",
                          "amp"),
      circular_variable = c("phi", "phi.begins6", "phi.begins12")
    ) %>%
      mutate(corr_data = map2(
        .x = syms(circular_variable),
        .y = syms(linear_variable),
        .f = ~ compute_correlations(eval_tidy(.x, data = cur_data()),
                                    eval_tidy(.y, data = cur_data()))
      )) %>%
      unnest(corr_data) %>%
      rename(par2 = circular_variable,
             par1 = linear_variable)
  ) %>%
  unnest(cols = last_col()) %>%
  pivot_longer(-1:-3,
               names_to = "cortypeXmeasure",
               values_to = "value")

beepr::beep(7)

cors_lin_lin <-  d_draws %>%
  group_by(iteration) %>%
  summarise(
    pair_data = list(
      c("mesor", "amp", "logsigma", "sigma", "sigma2", "bl_pa", "bl_na") %>% 
        combn(2) %>%
        t() %>%
        as.data.frame() %>%
        rename(par1 = V1, par2 = V2) %>% 
        tibble()
    )
  ) %>%
  unnest(pair_data) %>%
  mutate(
    x = map(par1, ~ pull(filter(d_draws, iteration == cur_group_id()), all_of(.))),
    y = map(par2, ~ pull(filter(d_draws, iteration == cur_group_id()), all_of(.))),
    pearson_cor = map2_dbl(x, y, ~ cor(.x, .y, use = "complete.obs")),
    pearson_pval = map2_dbl(x, y, ~ cor.test(.x, .y)$p.value),
    spearman_cor = map2_dbl(x, y, ~ cor(.x, .y, use = "complete.obs", method = "spearman")),
    spearman_pval = map2_dbl(x, y, ~ cor.test(.x, .y)$p.value, method = "spearman"),
    kendall_cor = map2_dbl(x, y, ~ cor(.x, .y, use = "complete.obs", method = "kendall")),
    kendall_pval = map2_dbl(x, y, ~ cor.test(.x, .y)$p.value, method = "kendall")
  ) %>%
  select(-x, -y) %>%
  unnest(cols = last_col()) %>%
  pivot_longer(-1:-3,
               names_to = "cortypeXmeasure",
               values_to = "value")

ests_level2 <- cors_lin_angle %>%
  rbind(cors_lin_lin) %>%
  group_by(cortypeXmeasure) %>%
  mutate(
    correlation_type = str_split(cortypeXmeasure, "_")[[1]][1],
    measure = str_split(cortypeXmeasure, "_")[[1]][2],
    .before = value
  ) %>%
  ungroup() %>%
  mutate(what = paste(par1, correlation_type, par2, sep = "_"),
         .after = correlation_type) %>%
  select(-cortypeXmeasure)

beepr::beep(9)

ests_level2 %>%
  cbind(item = item_, .) %>% 
saveRDS(here("fits",
             paste0("ests_level2_",
                    item_,
                    "_random_var.rds")
             )
        )

```

# Results

Before making the plots, we first read the saved results:

```{r}

draws <-
  rbind(
    readRDS(here("fits",
                 "draws_pa_random_var.rds")),
    readRDS(here("fits",
                 "draws_fitbit_random_var.rds"))
    )

el1 <- e_level1 <-
  rbind(
    readRDS(here("fits",
                 "ests_level1_pa_random_var.rds")),
    readRDS(here("fits",
                 "ests_level1_fitbit_random_var.rds"))
    )

el1.5e_level1.5 <- 
  rbind(
    readRDS(here("fits",
                 "ests_level1.5_pa_random_var.rds")),
    readRDS(here("fits",
                 "ests_level1.5_fitbit_random_var.rds"))
    )

el2 <- e_level2 <-
  rbind(
    readRDS(here("fits",
                 "ests_level2_pa_random_var.rds")),
    readRDS(here("fits",
                 "ests_level2_fitbit_random_var.rds"))
    )

```

## Level-1 plots

### Amplitude HDR

For the HDR plot we use `f_plot_hdr`, that plots the heatmap of the joint posterior distributions of $C_i$ and $S_i$, adds contour lines at 95, 95, 90, 85, and 80 %HDRs, and adds a line from the origin to the median of the joint distribution.

```{r}

f_plot_hdr <- function(df,
                       x_col = "co",
                       y_col = "si",
                       title = "HDR heatmap",
                       xlim = NULL,
                       ylim = NULL) {
  # Determine axis limits if not provided
  if (is.null(xlim) || is.null(ylim)) {
    co_range <- range(df[[x_col]], na.rm = TRUE)
    si_range <- range(df[[y_col]], na.rm = TRUE)
    max_range <- max(abs(c(co_range, si_range))) * 1.2  # Expand range by 20%
    xlim <- c(-max_range, max_range)
    ylim <- c(-max_range, max_range)
  }
  
  # Compute 2D density with extended limits
  density_est <- MASS::kde2d(df[[x_col]],
                             df[[y_col]],
                             n = 300,
                             lims = c(xlim[1], xlim[2], ylim[1], ylim[2]))  # Ensure density extends to plot edges
  
  # Convert density estimate to a dataframe
  density_df <- expand.grid(co = density_est$x, si = density_est$y) %>%
    mutate(density = as.vector(density_est$z)) %>%
    arrange(desc(density)) %>%
    mutate(cumulative_prob = cumsum(density) / sum(density) * 100)  # HDR
  
  # Compute median coordinates and distance from origin
  median_co <- median(df[[x_col]], na.rm = TRUE)
  median_si <- median(df[[y_col]], na.rm = TRUE)
  distance_from_origin <- sqrt(median_co^2 + median_si^2)
  
  # Define color palette
  viridis_colors <- viridis(256, direction = -1)
  viridis_colors[1] <- "#FFFFFF"  # Make the lowest density white
  
  # Generate tick labels for axes (without decimals)
  x_ticks <- seq(xlim[1], xlim[2], by = 5) %>% round()
  y_ticks <- seq(ylim[1], ylim[2], by = 5) %>% round()
  x_tick_labels <- ifelse(x_ticks == 0, "", as.character(x_ticks))
  y_tick_labels <- ifelse(y_ticks == 0, "", as.character(y_ticks))
  
  # Create plot
  p <- ggplot(density_df, aes(x = co, y = si)) +
    # Heatmap with fine-grained HDR levels
    geom_tile(aes(fill = cumulative_prob)) +
    # Reverse viridis_colors and set legend breaks at 10% intervals
    scale_fill_gradientn(
      colors = rev(viridis_colors),
      breaks = seq(0, 100, by = 10),
      minor_breaks = seq(0, 100, by = 5),
      limits = c(0, 100),
      name = "HDR (%)"
    ) +
    # Add contour lines at specified HDR levels
    geom_contour(
      aes(z = cumulative_prob),
      breaks = c(95, 90, 85, 80),
      color = "white",
      size = 0.7
    ) +
    # Add custom axes at the center
    geom_hline(yintercept = 0,
               color = "black",
               alpha = 0.6,
               size = 0.4) +  # X-axis
    geom_vline(xintercept = 0,
               color = "black",
               alpha = 0.6,
               size = 0.4) +  # Y-axis
    # Add line segments for median projections
    annotate(
      "segment",
      x = median_co,
      xend = median_co,
      y = 0,
      yend = median_si,
      color = "blue",
      linetype = "dashed"
    ) +  # Vertical projection
    annotate(
      "segment",
      x = 0,
      xend = median_co,
      y = median_si,
      yend = median_si,
      color = "blue",
      linetype = "dashed"
    ) +  # Horizontal projection
    annotate(
      "segment",
      x = 0,
      xend = median_co,
      y = 0,
      yend = median_si,
      color = "red",
      size = 0.7
    ) +  # Line from origin to median
    # Ensure square aspect ratio
    coord_fixed(ratio = 1,
                xlim = xlim,
                ylim = ylim) +
    # Theme and labels
    theme_few() +
    labs(x = TeX("$C_i$"), y = TeX("$S_i$"), title = title)
  
  return(p)
}
```

And make plots for Persons 76 and 179 of Dataset 1 (with the origin at 75.0%HDR and 98.9%HDR, respectively) of Dataset 1, and Persons 11 and 42 of Dataset 2 (with the origin at 83.9%HDR and 100%HDR, respectively).

```{r}

p_pa_76 <- draws %>%
  filter(item == "pa", id == 76) %>%
  f_plot_hdr(title = "Person D1-76")
p_pa_179 <- draws %>%
  filter(item == "pa", id == 179) %>%
  f_plot_hdr(title = "Person D1-179")

p_fitbit_11 <- draws %>%
  filter(item == "fitbit", id == 11) %>%
  f_plot_hdr(title = "Person D2-11")
p_fitbit_42 <- draws %>%
  filter(item == "fitbit", id == 42) %>%
  f_plot_hdr(title = "Person D2-42")


p_four <- (p_pa_76/ p_pa_179) | (p_fitbit_11/ p_fitbit_42)

(p_four +
  plot_layout(guides = "collect") &
  theme(legend.position = 'bottom',
        legend.key.width = unit(1.5, "cm"))) # + plot_annotation("Posterior joint distribution of C_i and S_i")

ggsave("joint-posterior-hdr.pdf",
       width = 20,
       height = 20,
       units = "cm")
```

We print the person-specific amplitude and the corresponding HDR level of each individual, with percentage of individuals that fall within each HDR level:

```{r}

library(ggplot2)
library(dplyr)
library(viridis)

# Assuming el1_tmp is your dataframe
el1_tmp <- el1 %>%
  mutate(interval = cut(
    amp_hdr,
    breaks = c(0, 80, 85, 90, 95, 100),
    labels = c("0-80", "80-85", "85-90", "90-95", "95-100"),
    include.lowest = TRUE,
    ordered_result = TRUE
  ),
  item = case_when(
    item == "pa" ~ "Dataset 1",
    TRUE ~ "Dataset 2"
  ))

# Generate a complete set of intervals for each item to ensure missing intervals are represented
interval_template <- expand.grid(
  item = unique(el1_tmp$item),
  interval = levels(el1_tmp$interval),
  stringsAsFactors = FALSE
)

# Calculate percentages per interval per item
interval_counts <- el1_tmp %>%
  group_by(item) %>%
  mutate(total_per_item = n()) %>%
  group_by(item, interval) %>%
  summarise(n = n(),
            percent = round(n / first(total_per_item) * 100, 1),
            .groups = 'drop') %>%
  right_join(interval_template, by = c("item", "interval")) %>%
  mutate(n = replace_na(n, 0),
         percent = replace_na(percent, 0))

# Adjust interval y positions
interval_counts <- interval_counts %>%
  mutate(
    y_position = case_when(
      interval == "0-80" ~ 77.5,
      interval == "80-85" ~ 82.5,
      interval == "85-90" ~ 87.5,
      interval == "90-95" ~ 92.5,
      interval == "95-100" ~ 97.5
    )
  )

# Calculate max amp_median per item to position labels appropriately with better spacing
max_amp_median <- el1_tmp %>% 
  group_by(item) %>% 
  summarise(max_median = max(amp_median, na.rm = TRUE))

interval_counts <- interval_counts %>%
  left_join(max_amp_median, by = "item") %>%
  mutate(label_x_position = max_median * 1.015)  # Adjusted to a smaller shift

p_hist <- ggplot(el1_tmp, aes(x = amp_median)) +
  geom_histogram(alpha = 0.6,
                 fill = "cornflowerblue") +
  theme_minimal() +
  labs(x = "", y = "") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, NA)) +
  facet_wrap( ~ item,
              nrow = 1,
              scales = "free",
              strip.position = "top") +
  theme(panel.spacing = unit(2, "lines"),
        strip.text = element_text(size = 12),
        legend.position = "none",
        axis.text = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        panel.background = element_blank(),
        plot.margin = margin(0,0,0,0)) 


# Main scatter plot with facet wrap by item and adjusted spacing
p_scatter <- ggplot(el1_tmp, aes(x = amp_median,
                     y = amp_hdr,
                     color = interval)) +
  geom_point(alpha = 0.6) +
  geom_hline(
    yintercept = c(95, 90, 85, 80),
    linetype = "dotted",
    color = "red"
  ) +
  scale_color_viridis_d(begin = 0.3, end = 0.95) +
  scale_fill_viridis_d(begin = 0.3, end = 0.95) +
  theme_minimal() +
  labs(x = "Person-specific Amplitude", y = TeX("$\\alpha$%HDR")) +
  geom_label(
    data = interval_counts,
    aes(
      x = label_x_position,
      y = y_position,
      label = paste0(percent, "%"),
      fill = interval
    ),
    alpha = 0.6,
    color = "black",
    hjust = 1.1,
    size = 4
    # label.size = 0  # Removes border for better readability
  ) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, NA)) +
  scale_y_continuous(breaks = seq(0, 100, 25) %>% c(80, 85, 90, 95)) +
  facet_wrap( ~ item,
              nrow = 1,
              scales = "free_x",
              strip.position = "top") +
  theme(panel.spacing = unit(2, "lines"),
        strip.text = element_blank(),#element_text(size = 12),
        legend.position = "none")


p_hist / p_scatter + patchwork::plot_layout(heights = c(0.5,3))


ggsave("l1-amp-hist-with-hdr.pdf",
       width = 20,
       height = 25,
       units = "cm")
```
