---
title: "Level-1 and level-2 estimates of the multilevel cosinor model"
author: "MH Manuel Haqiqatkhah"
date: 2024-12-10
date-modified: last-modified
date-format: "YYYY-MM-DD"
format: html
execute:
  eval: false
  echo: true
editor: visual
---

# Intro

In this repository, I will fit explore multilevel modeling of the cosinor model, starting with single-component cosinor model on ESM data from Leuven Wave 1 dataset.

The raw data is located in the `data` folder, and the long version of it is saved in `./data/d_leuven_long.rsd`.
This dataframe contains variables `id` (between 1 and 202), `wave` (only 1), `beep_num` (number of beep throughout study, from 1 to 70), `hour_in_day` (clock hour in base-10, between 0 and 24), `beep_in_day` (beep number of the measurement day, from 1 to 10), `t` (date + time), `Date_Local` (dd-mm-yy), `Time_Local` (clock hour in h:m:s), `item` (12 items: `pa`, `na`, `ang`, `cheer`, `conf`, `dep`, `fear`, `hap`, `lone`, `rlx`, `sad`, `str`), `y` (measured value, from 0 to 100), and `time_in_hours` (decimal time since start of the data collection).

The approach is to first fit the cosinor model (with the linear transformation) to the happiness time series using `brms`, then do our stuff (that follows).

# Analysis

The required packages are as follows:

```{r}

library(brms)
library(psych)
library(Rfast)
library(tidyverse)
library(ggthemes)
library(here)
library(ggh4x)
library(rlang)
library(data.table)
library(circular)

```

## Fitting the cosinor model

We make a new dataframe `d` by first filtering the data to only scores from one item denoted by `item_`; we consider `hap`, `sad`, `pa`, and `na` for `item_`, starting by `item_ <- "hap"`.
We then change the name of the time variable (clock hour in day) to `t`, and add `co` ($\cos(\frac{2\pi}{24}t)$) and `si` ($\sin(\frac{2\pi}{24}t)$) as predictors:

```{r}
#| label:read-data

item_ <- "na"

d <- d_l %>%
  filter(item == item_) %>%
  mutate(t = hour_in_day,
         co = cos(2*pi/24 * t),
         si = sin(2*pi/24 * t)) %>%
  select(id, t, y, co, si)
```

Then we fit two versions of the cosinor model: One with residual variance estimated as fixed effects (i.e., everyone getting the same $\sigma^2$):

```{r}
#| label: brms-fixed-residual-variance

(st <- Sys.time())
m_fixed_var <- brm(
  y ~ 1 + co + si | id,
  data = d,
  backend = "cmdstanr",
  chains = 4,
  iter = 3000,
  threads = threading(2),
  cores = 8)
Sys.time() - st
beepr::beep(1)

saveRDS(m_fixed_var,
        here("fits",
            paste0("brms_",
                   item_,
                   "_fixed_var.rds")
            )
        )
```

And one with random residual variance:

```{r}
#| label: brms-random-residual-variance

(st <- Sys.time())
m_random_var <- brm(
  brmsformula(
    y ~ 1 + co + si | id,
    sigma ~ 1 | id),
  data = d,
  backend = "cmdstanr",
  chains = 4,
  iter = 3000,
  threads = threading(2),
  cores = 8)
Sys.time() - st
beepr::beep(3)

saveRDS(m_random_var,
        here("fits",
             paste0("brms_",
                    item_,
                    "_random_var.rds")
             )
        )
```

And extract person-specific samples (per individual `id` $i$ and sample `iteration` sample $s$) of random effects: `mesor` (i.e., $M_i^s$ from `Intercept)`, `co` (i.e, ${\beta_c}_i^s$), and `si` (i.e., ${\beta_s}_i^s$).
For the second model, we also extract `sigmaIntercept` (i.e., $\ln(\sigma_i^s)$, initially stored in `r_id__sigma[.,Intercept]`, also store it in `log_sigma`) and `sigma2` (i.e., ${\sigma_i^s}^2$ from `exp(sigmaIntercept)^2`.

So for each individual `id` $i$ and sample `iteration` sample $s$, we calculate the correct `phi` ($\phi_i^s = \text{atan2}({\beta_s}_i^s, {\beta_c}_i^s)$) and incorrect `phi_atan` ($\phi_i^s = \text{atan}({\beta_s}_i^s/{\beta_c}_i^s)$) and the corresponding peak shifts $\psi_i^s = \frac{24}{2\pi} \phi_i^s$.
We further add `phi.begins6` ($[\phi_i^s - \pi/2] \text{ mod } 2\pi$) and `phi.begins12` ($[\phi_i^s - \pi] \text{ mod } 2\pi$) as well, which are the correctly calculated $\phi_i^s$, but assuming the day starts at 6:00 and 12:00, respectively.

From now on, we move on with the estimates of the second model; for the first model, remove `sigma`... lines in the code below.

```{r}
#| label: extract-samples

# Selecting the second model from now on
m_fit <- m_random_var

# Taking out the individual-specific sampled parameter estimates
d_draws <- as_draws_df(m_fit) %>%
  select(contains("r_id[") | contains("r_id__sigma[")) %>%
  mutate(iteration = 1:n()) %>%
  pivot_longer(
    cols = starts_with("r_id"),
    names_to = "id_x_variable",
    values_to = "value"
  ) %>%
  # Extract id (number between [ and ,)
  mutate(
    id_x_variable =
      str_replace(id_x_variable,
                  "r_id__sigma\\[(\\d+),Intercept\\]",
                  "r_id__sigma[\\1,sigmaIntercept]"),
    id = str_extract(id_x_variable,
                     "(?<=\\[)[0-9]+(?=,)") %>%
         as.numeric(),
    # Extract parameter (characters between , and ])
    parameter = str_extract(id_x_variable,
                            "(?<=,)[^\\]]+(?=\\])"),
    # Clean up parameter by removing whitespace
    parameter = str_trim(parameter)
  ) %>%
  # Remove original variable column and reorder
  select(iteration, id, parameter, value) %>%
  pivot_wider(names_from = "parameter",
              values_from = "value") %>%
  mutate(
    ## Not sure if sigmaIntercept is on log scale or not
    ## Commenting it out now
    log_sigma = sigmaIntercept,
    sigma2 = exp(sigmaIntercept)^2,
    mesor = Intercept,
    amp = sqrt(si^2 + co^2),
    phi = atan2(si, co) %% (2*pi),
    phi_atan = atan(si/co) %% (2*pi),
    psi = phi*24/(2*pi),
    psi_atan = phi_atan*24/(2*pi)
    ) %>%
  mutate(phi.begins6 = (phi - pi/2) %% (2*pi),
         phi.begins12 = (phi - pi) %% (2*pi))

beep(5)
```

## Extracting estimates

### Level-1 estimates

To get person-specific summary statistics, we do calculations across iterations (i.e., only `group_by(id) %>% summarize(...)`) to get mean, median, and 95%CI width of our estimated linear parameters, and for the circular parameters (`phi`, `phi.begins6`, `phi.begins12`), once incorrectly (using linear methods) and once correctly (using circular methods).

```{r}
#| label: extract-level1-estimates

# Helper function for linear stats
compute_linear_stats <- function(data, variable) {
  tibble(
    mean = mean(data[[variable]], na.rm = TRUE),
    median = median(data[[variable]], na.rm = TRUE),
    ci_width = quantile(data[[variable]], 0.975, na.rm = TRUE) - quantile(data[[variable]], 0.025, na.rm = TRUE)
  ) %>%
    rename_with(~ paste0(variable, "_", .))
}

# Helper function for circular stats
compute_circular_stats <- function(data, variable) {
  tibble(
    circ_mean = (mean.circular(data[[variable]]) %% (2 * pi)) %>% as.numeric(),
    circ_median = (median.circular(data[[variable]]) %% (2 * pi)) %>% as.numeric(),
    circ_ci_width = (
      (quantile.circular(data[[variable]], 0.975) - quantile.circular(data[[variable]], 0.025)) %% (2 * pi)
    ) %>% as.numeric()
  ) %>%
    rename_with(~ paste0(variable, "_", .))
}

# Apply calculations for all variables
ests_level1 <- d_draws %>%
  group_by(id) %>%
  summarise(
    bind_cols(
      # Linear calculations
      compute_linear_stats(cur_data(), "mesor"),
      compute_linear_stats(cur_data(), "si"),
      compute_linear_stats(cur_data(), "co"),
      compute_linear_stats(cur_data(), "amp"),
      compute_linear_stats(cur_data(), "sigma2"),
      compute_linear_stats(cur_data(), "log_sigma"),
      compute_linear_stats(cur_data(), "phi"),
      compute_linear_stats(cur_data(), "phi.begins6"),
      compute_linear_stats(cur_data(), "phi.begins12"),

      # Circular calculations
      compute_circular_stats(cur_data(), "phi"),
      compute_circular_stats(cur_data(), "phi.begins6"),
      compute_circular_stats(cur_data(), "phi.begins12")
    )
  )

saveRDS(ests_level1,
        here("fits",
             paste0("ests_level1_",
                    item_,
                    "_random_var.rds")
             )
        )

beep(10)

```

To speed up, we can use `data.table` (and parallelization) with different definitions for `compute_` functions, and of course data.table operations (note that on Windows, you should set `mc.cores = 1`, meaning no parallelization with this code):

```{r}
#| label: extract-level1-estimates-datatable

# Load data.table library
library(data.table)

# Helper function for linear stats
compute_linear_stats <- function(data, variable) {
    setNames(
        list(
            mean(data[[variable]], na.rm = TRUE),
            median(data[[variable]], na.rm = TRUE),
            quantile(data[[variable]], 0.975, na.rm = TRUE) - quantile(data[[variable]], 0.025, na.rm = TRUE)
        ),
        paste0(variable, c("_mean", "_median", "_ci_width"))
    )
}

# Helper function for circular stats
compute_circular_stats <- function(data, variable) {
    setNames(
        list(
            (mean.circular(data[[variable]]) %% (2 * pi)) %>% as.numeric(),
            (median.circular(data[[variable]]) %% (2 * pi)) %>% as.numeric(),
            ((quantile.circular(data[[variable]], 0.975) - quantile.circular(data[[variable]], 0.025)) %% (2 * pi)) %>% as.numeric()
        ),
        paste0(variable, c("_circ_mean", "_circ_median", "_circ_ci_width"))
    )
}

# Function to compute stats for one group (one id)
compute_stats <- function(dt) {
  dt[, {
    # Linear variables
    linear_stats <- unlist(lapply(
      c("mesor", "si", "co", "amp", "sigma2", "log_sigma", "phi", "phi.begins6", "phi.begins12"),
      function(var) compute_linear_stats(.SD, var)
    ), recursive = FALSE)
    
    # Circular variables
    circular_stats <- unlist(lapply(
      c("phi", "phi.begins6", "phi.begins12"),
      function(var) compute_circular_stats(.SD, var)
    ), recursive = FALSE)
    
    # Combine stats
    c(linear_stats, circular_stats)
  }, by = id]
}

# Convert to data.table
d_draws_dt <- as.data.table(d_draws)

# Split data by id for parallel processing
data_split <- split(d_draws_dt, by = "id")

# Apply calculations in parallel
ests_level1_dt <- rbindlist(mclapply(data_split, compute_stats, mc.cores = detectCores() - 1))
```

### Level-2 estimates

Importantly, we want to estimate linear and correlations between mesor, amplitude, and residual variance across individuals for each sample.

To do so, we need to first define a function to calculate Mardia's circular-linear rank correlation:

```{r}

# Mardia circular-linear rank correlation
cor_mardia <- function(theta, x) {
  # Remove NAs
  valid <- complete.cases(theta, x)
  theta <- theta[valid]
  x <- x[valid]
  n <- length(theta)
  if (n == 0) stop("No valid data after removing NAs.")
  
  # Rank and compute theta star
  r_theta <- rank(theta, ties.method = "average")
  r_x <- rank(x, ties.method = "average")
  r_theta_star <- r_theta * 2 * pi / n
  
  # Precompute sine and cosine
  cos_theta_star <- cos(r_theta_star)
  sin_theta_star <- sin(r_theta_star)
  
  # Compute T_c and T_s
  T_c <- sum(r_x * cos_theta_star)
  T_s <- sum(r_x * sin_theta_star)
  
  # Calculate coefficient 'a'
  a <- ifelse(
    n %% 2 == 0,
    1 / (1 + 5 / tan(pi / n)^2 + 4 / tan(pi / n)^4),
    2 * sin(pi / n)^4 / (1 + cos(pi / n))^3
  )
  
  # Final D value (rank correlation, values between 0 and 1)
  D <- a * (T_c^2 + T_s^2)
  
  # Calculate U_n and p-value
  U_n <- 24 * (T_c^2 + T_s^2) / (n^2 * (n + 1))
  p.value <- 1 - pchisq(U_n, df = 2)
  
  # Return results as a list
  return(list(estimate = D, p.value = p.value, U_n = U_n))
}
```

To make the code cleaner, we use a helper function that, for correlations where $\phi$ is involved, calculates various correlations (Pearson's, Spearman's rank, Kendall's rank, as well as Johnson-Wehrly-Mardia, and Mardia's rank) for us:

```{r}

# Create correlation function with linear and circular methods
compute_correlations <- function(x, y) {
  
  ## x is a circular variable, y is a linear variable
  cor_mardia <- cor_mardia(x, y)
  
  tibble(
    ## Linear correlations
    # Pearson's
    pearson_cor = cor(x, y),
    pearson_pval = cor.test(x, y)$p.value,
    # Spearman's
    spearman_cor = cor(x, y, method = "spearman"),
    spearman_pval = cor.test(x, y, method = "spearman")$p.value,# Kendall's tau
    kendall_cor = cor(x, y, method = "kendall"),
    kendall_pval = cor.test(x, y, method = "kendall")$p.value,
    ## Circular correlations
    # Johnson-Wehrly-Mardia correlation (sinusoidal assumption)
    jwm_cor = tryCatch(
      circlin.cor(x, y)[1],
      error = function(e)
        NA_real_
    ),
    jwm_pval = tryCatch(
      circlin.cor(x, y)[2],
      error = function(e)
        NA_real_
    ),
    # Mardia's rank correlation (no assumption)
    mard_cor = cor_mardia$estimate,
    mard_pval = cor_mardia$p.value
  )
}
```

We then use the functions to do the calculations, make a long dataframe, and save it as an `.RDS` file:

```{r}
#| label: extract-level2-estimates

# Compute correlations for all combinations
cors_lin_angle <- d_draws %>%
  group_by(iteration) %>%
  summarise(
    crossing(
      linear_variable = c('mesor', 'log_sigma', 'sigma2', 'amp'),
      circular_variable = c('phi', 'phi.begins6', 'phi.begins12')
    ) %>%
      mutate(corr_data = map2(
        .x = syms(circular_variable),
        .y = syms(linear_variable),
        .f = ~ compute_correlations(eval_tidy(.x, data = cur_data()),
                                    eval_tidy(.y, data = cur_data()))
      )) %>%
      unnest(corr_data) %>%
      rename(par2 = circular_variable,
             par1 = linear_variable)
  ) %>%
  unnest(cols = last_col()) %>%
  pivot_longer(-1:-3,
               names_to = "cortypeXmeasure",
               values_to = "value")

beep(7)

cors_lin_lin <- d_draws %>%
  group_by(iteration) %>%
  summarise(
    pair_data = list(
      tibble(
        par1 = c("mesor", "mesor", "mesor", "amp", "amp"),
        par2 = c("amp", "log_sigma", "sigma2", "log_sigma", "sigma2")
      )
    )
  ) %>%
  unnest(pair_data) %>%
  mutate(
    x = map(par1, ~ pull(filter(d_draws, iteration == cur_group_id()), all_of(.))),
    y = map(par2, ~ pull(filter(d_draws, iteration == cur_group_id()), all_of(.))),
    pearson_cor = map2_dbl(x, y, ~ cor(.x, .y, use = "complete.obs")),
    pearson_pval = map2_dbl(x, y, ~ cor.test(.x, .y)$p.value),
    spearman_cor = map2_dbl(x, y, ~ cor(.x, .y, use = "complete.obs", method = "spearman")),
    spearman_pval = map2_dbl(x, y, ~ cor.test(.x, .y)$p.value, method = "spearman"),
    kendall_cor = map2_dbl(x, y, ~ cor(.x, .y, use = "complete.obs", method = "kendall")),
    kendall_pval = map2_dbl(x, y, ~ cor.test(.x, .y)$p.value, method = "kendall")
  ) %>%
  select(-x, -y) %>%
  unnest(cols = last_col()) %>%
  pivot_longer(-1:-3,
               names_to = "cortypeXmeasure",
               values_to = "value")

ests_level2 <- cors_lin_angle %>%
  rbind(cors_lin_lin) %>%
  group_by(cortypeXmeasure) %>%
  mutate(
    correlation_type = str_split(cortypeXmeasure, "_")[[1]][1],
    measure = str_split(cortypeXmeasure, "_")[[1]][2],
    .before = value
  ) %>%
  ungroup() %>%
  mutate(what = paste(par1, correlation_type, par2, sep = "_"),
         .after = correlation_type) %>%
  select(-cortypeXmeasure)

beep(9)

saveRDS(ests_level2,
        here("fits",
             paste0("ests_level2_",
                    item_,
                    "_random_var.rds")
             )
        )

```

# Results

Before making the plots, we first read the saved results:

```{r}

ests_level1 <- readRDS(here("fits",
                            paste0("ests_level1_",
                                   item_,
                                   "_random_var.rds")))
ests_level2 <- readRDS(here("fits",
                            paste0("ests_level2_",
                                   item_,
                                   "_random_var.rds")))
```

## Level-1 plots

```{r}

```
