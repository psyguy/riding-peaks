---
title: "Level-1 and level-2 estimates of the multilevel cosinor model"
author: "MH Manuel Haqiqatkhah"
date: 2024-12-10
date-modified: last-modified
date-format: "YYYY-MM-DD"
format: html
execute:
  eval: false
  echo: true
editor: visual
bibliography: references.bib
---

# Intro

In this repository, I will fit explore multilevel modeling of the cosinor model, starting with single-component cosinor model fitted to positive affect (PA) ESM time series data from Leuven Wave 1 of @erbas_2018_WhyDonAlways, and heart rate (fitbit) data from @yfantidou_2022_LifeSnaps4monthMultimodal. The processed datasets (saved as `d_leuven_joined.rds` and `d_fitbit_joined.rds` in the `data` folder) contains ILD measured values in `y`, the base-10 clock hour (between 0 and 24) in `t`, the regressors `co` ($\cos(\frac{2\pi}{24}t)$) and `si` ($\sin(\frac{2\pi}{24}t)$) calculated for each `t`, baseline PA (`b_pa`), and baseline NA (`bl_na`).

The required packages are as follows:

```{r}

library(brms)
library(psych)
library(Rfast)
library(MASS)
library(tidyverse)
library(patchwork)
library(ggthemes)
library(viridis)
library(latex2exp)
library(here)
library(ggh4x)
library(ggforce)
library(rlang)
library(data.table)
library(circular)

```

# Analyses

We load the datasets, and run the analyses that follow for each separately.

```{r}
#| label: read-data

# For the Leuven dataset; choose pa
item_ <- "pa"
d <- readRDS(here("data",
                  "d_leuven_joined.rds")) %>%
  filter(item == item_)

# For fitbit dataset
item_ <- "fitbit"
d <- readRDS(here("data",
                  "d_fitbit_joined.rds"))


# Get the baseline PA and NA for the selected dataset
d_bl <- d %>%
  select(id, bl_pa, bl_na) %>% 
  distinct()

```

## Fitting the cosinor model

We fit two versions of the cosinor model.

### Model with fixed residual variance

One with residual variance estimated as fixed effects (i.e., everyone getting the same $\sigma^2$), that is,

$$
\begin{aligned}
&M_i = M + m_i \\
&C_i = C + c_i \\
&S_i = S + c_i \\
&\sigma_i = \sigma
\end{aligned} \quad,
$$

where

$$
\begin{bmatrix}M_i \\C_i \\S_i \end{bmatrix}
\sim\mathcal{MN}(
\begin{bmatrix}M \\C \\S \end{bmatrix}
,\mathbf{\Sigma})
$$

using the following `brms` code:

```{r}
#| label: brms-fixed-residual-variance

(st <- Sys.time())
m_fixed_var <- brm(
  y ~ 1 + co + si + (1 + co + si | id),
  data = d,
  backend = "cmdstanr",
  chains = 4,
  iter = 3000,
  threads = threading(2),
  cores = 8)
Sys.time() - st
beepr::beep(1)

saveRDS(m_fixed_var,
        here("fits",
            paste0("brms_",
                   item_,
                   "_fixed_var.rds")
            )
        )
```

### Model with random residual variance

We also fit a model with random residual variance, in which we also include an equation for the log of the individual specific variances $\rho_i = \log\sigma_i$ and the log of the fixed variance are $\rho = \log\sigma$, leading to

$$
\begin{aligned}
&M_i = M + m_i \\
&C_i = C + c_i \\
&S_i = S + c_i \\
&\rho_i = \rho + \varrho_i
\end{aligned} \quad ,
$$

with

$$
\begin{bmatrix}M_i \\C_i \\S_i \\\rho_i\end{bmatrix}
\sim\mathcal{MN}(
\begin{bmatrix}M \\C \\S \\\rho\end{bmatrix}
,\mathbf{\Sigma})
$$

using the following `brms` code:

```{r}
#| label: brms-random-residual-variance

(st <- Sys.time())
m_random_var <- brm(
  brmsformula(
    y ~ 1 + co + si + (1 + co + si | i | id),
    sigma ~ 1 + (1 | i | id)),
  data = d,
  backend = "cmdstanr",
  chains = 4,
  iter = 3000,
  threads = threading(2),
  cores = 8)
Sys.time() - st
beepr::beep(3)

saveRDS(m_random_var,
        here("fits",
             paste0("brms_",
                    item_,
                    "_random_var.rds")
             )
        )
```

Note that `| i | id` in the random effect specification sets the grouping variable `id` and assures to estimate the correlation between the random intercept and random slope in the first equation (for `y`) and the random intercept in the second equation (for `sigma`). We move on doing the analyses with the second model, so,

```{r}

# Model to move forward with
m_fit <- readRDS(here("fits",
             paste0("brms_",
                    item_,
                    "_random_var.rds")))
```

### Model outputs

The model `m_fit` contains samples from $D=6,000$ HMC draws, that include $4 \times D$ values for the fixed effects $M^{(d)}$, $C^{(d)}$, $S^{(d)}$, and $\rho^{(d)}$ (reported, respectively, as `b_Intercept`, `b_co`, `b_si`, and `b_sigma_Intercept`), $(6+4) \times D$ values for the correlations and SDs of $\mathbf{\Sigma}^{(d)}$, and $4 \times N \times D$ person-specific the random effects $m_i^{(d)}$, $c_i^{(d)}$, $s_i^{(d)}$, and $\varrho_i^{(d)}$ (reported, respectively, as `Intercept`, `co`, `si`, and `sigma_Intercept`).

From these, at each draw $d$, we calculate the person-specific parameters $M_i^{(d)}$, $C_i^{(d)}$, $S_i^{(d)}$, and $\rho_i^{(d)}$ by summing the fixed and random effects and storing them, respectively, in `mesor`, `co`, `si` and `logsigma` columns. Based on these, we calculate person-specific SD (`sigma = exp(logsigma)`) and variance (`sigma2 = exp(logsigma)^2`), and the implied amplitude `amp` and phase (correctly calculated with `atan2(si,co)` in `phi`, and incorrectly calculate using `atan(si/co)` in `phi_atan`), and do these transformations for fixed effects (the `b_` variables). We also calculate the phase assuming starting the cycle, instead of midnight, at 6:00 (`phi.begins6`) and at 12:00 (`phi.begins12`).

```{r}
#| label: extract-samples

# Taking out the individual-specific sampled parameter estimates
d_draws <- as_draws_df(m_fit) %>%
mutate(iteration = row_number()) %>%
  pivot_longer(
    cols = starts_with("r_id"),
    names_pattern = "r_id(_{0,2}sigma)?\\[(\\d+),(.*)\\]",
    names_to = c("sigma_part", "id", "term"),
    values_to = "value"
  ) %>%
  mutate(term = if_else(sigma_part == "__sigma", paste0("sigma_", term), term)) %>%
  pivot_wider(
    id_cols = c(iteration, id, b_Intercept, b_co, b_si, b_sigma_Intercept),
    names_from = term,
    values_from = value
  ) %>%
  mutate(
    mesor = Intercept + b_Intercept,
    co = co + b_co,
    si = si + b_si,
    logsigma = sigma_Intercept + b_sigma_Intercept
  ) %>%
  select(iteration, id,
         mesor, co, si, logsigma,
         b_Intercept, b_co, b_si, b_sigma_Intercept) %>%
  rename(b_logsigma = b_sigma_Intercept) %>%
  mutate(
    sigma = exp(logsigma),
    sigma2 = exp(logsigma)^2,
    amp = sqrt(si^2 + co^2),
    phi = atan2(si, co) %% (2*pi),
    phi_atan = atan(si/co) %% (2*pi),
    b_amp = sqrt(b_si^2 + b_co^2),
    b_phi = atan2(b_si, b_co) %% (2*pi),
    b_phi_atan = atan(b_si/b_co) %% (2*pi)
  ) %>%
  mutate(phi.begins6 = (phi - pi/2) %% (2*pi),
         phi.begins12 = (phi - pi) %% (2*pi)) %>% 
  mutate(id = id %>% as.integer())

d_draws %>%
  cbind(item = item_, .) %>%
  saveRDS(
    here("fits",
         paste0("draws_", item_, "_random_var.rds"))
    )

beepr::beep(5)
```

## Obtaining estimates

### Level-1 estimates

To get person-specific summary statistics, we do calculations across iterations (i.e., only `group_by(id) %>% summarize(...)`) to get mean, median, and 95%CI width of our estimated linear parameters, and for the circular parameters (`phi`, `phi.begins6`, `phi.begins12`), once incorrectly (using linear methods) and once correctly (using circular methods).

We also need to get the complementary of posterior predictive p-value using the percentage of density in which $[0,0]$ falls by finding the $\alpha\%\text{-HDR}$ (highest density region) for `amp` based on samples of `co` and `si`, which we do with `compute_alpha_hdr`:

```{r}
#| label: compute_alpha_hdr

# Load required packages
require(ks)
require(ggplot2)
require(metR)

compute_alpha_hdr <-
  function(d,
           x_0 = 0,
           y_0 = 0,
           n_grid = 200){
    # 1. Kernel density estimation
    fhat <- kde(d, gridsize = c(n_grid, n_grid))
    
    xseq <- fhat$eval.points[[1]]
    yseq <- fhat$eval.points[[2]]
    zmat <- fhat$estimate
    
    dx <- diff(xseq)[1]
    dy <- diff(yseq)[1]
    cell_area <- dx * dy
    
    # 2. Compute cumulative probability distribution to determine HDR thresholds
    dens_vec <- as.vector(zmat)
    dens_sorted <- sort(dens_vec, decreasing = TRUE)
    cumulative_mass <- cumsum(dens_sorted * cell_area)
    cumulative_mass <-
      cumulative_mass / max(cumulative_mass)  # normalize
    
    # 3. Evaluate density at the given point (x_0, y_0)
    f_point <- predict(fhat, x = cbind(x_0, y_0))
    
    # Find where f_point fits in the density distribution
    # Identify the position of f_point in the sorted densities
    idx <- which(dens_sorted < f_point)[1]
    
    if (is.na(idx)) {
      # f_point is >= all densities, alpha_hdr ~ 1
      alpha_hdr <- 1
    } else {
      # f_point lies between dens_sorted[idx-1] and dens_sorted[idx]
      # if idx > 1, cumulative_mass[idx-1] gives us a close approximation
      if (idx > 1) {
        alpha_hdr <- cumulative_mass[idx - 1]
      } else {
        # idx = 1 means f_point < dens_sorted[1], so alpha is slightly less than cumulative_mass[1]
        alpha_hdr <- cumulative_mass[1]
      }
    }
    
    return(round(alpha_hdr * 100, 1))
  }
```

We then use this function along other ones

```{r}
#| label: extract-level1-estimates

# Helper function for linear stats
compute_linear_stats <- function(data, variable) {
  tibble(
    mean = mean(data[[variable]], na.rm = TRUE),
    median = median(data[[variable]], na.rm = TRUE),
    ci_width = quantile(data[[variable]], 0.975, na.rm = TRUE) - quantile(data[[variable]], 0.025, na.rm = TRUE)
  ) %>%
    rename_with(~ paste0(variable, "_", .))
}

# Helper function for circular stats
compute_circular_stats <- function(data, variable) {
  
  ci_2.5 <- quantile.circular(data[[variable]], 0.025)
  ci_97.5 <- quantile.circular(data[[variable]], 0.975)
  # if(ci_2.5 > ci_97.5) ci_2.5 <- ci_2.5 - 2*pi
  
  tibble(
    circ_mean = (mean.circular(data[[variable]]) %% (2 * pi)) %>% as.numeric(),
    circ_median = (median.circular(data[[variable]]) %% (2 * pi)) %>% as.numeric(),
    circ_ci_2.5 = (ci_2.5 %% (2 * pi)) %>% as.numeric(),
    circ_ci_97.5 = (ci_97.5 %% (2 * pi)) %>% as.numeric(),
    circ_ci_width = (
      (ci_97.5 - ci_2.5) %% (2 * pi)
    ) %>% as.numeric()
  ) %>%
    rename_with(~ paste0(variable, "_", .))
}

# Apply calculations for all variables
ests_level1 <- d_draws %>%
  # Change this to group_by(iteration) for level1.5
  group_by(id) %>%
  summarise(
    bind_cols(
      # Linear calculations
      compute_linear_stats(cur_data(), "mesor"),
      compute_linear_stats(cur_data(), "si"),
      compute_linear_stats(cur_data(), "co"),
      compute_linear_stats(cur_data(), "amp"),
      compute_linear_stats(cur_data(), "logsigma"),
      compute_linear_stats(cur_data(), "sigma"),
      compute_linear_stats(cur_data(), "sigma2"),
      compute_linear_stats(cur_data(), "phi"),
      compute_linear_stats(cur_data(), "phi.begins6"),
      compute_linear_stats(cur_data(), "phi.begins12"),
      # Circular calculations
      compute_circular_stats(cur_data(), "phi"),
      compute_circular_stats(cur_data(), "phi.begins6"),
      compute_circular_stats(cur_data(), "phi.begins12")
    ),
    # Compute amp_hdr using compute_alpha_hdr
    amp_hdr = compute_alpha_hdr(cbind(cur_data()$co, cur_data()$si))
  ) %>%
  relocate(amp_hdr, .before = logsigma_mean)

ests_level1 %>%
  full_join(d_bl, by = "id") %>%
  cbind(item = item_, .) %>%
  saveRDS(
    here("fits",
         paste0("ests_level1_", item_, "_random_var.rds"))
    )

beepr::beep(10)

```

### Level-2 estimates

Importantly, we want to estimate linear and correlations between MESOR, amplitude, and residual variance across individuals for each sample.

To do so, we need to first define a function to calculate Mardia's circular-linear rank correlation:

```{r}

# Mardia circular-linear rank correlation
cor_mardia <- function(theta, x) {
  # Remove NAs
  valid <- complete.cases(theta, x)
  theta <- theta[valid]
  x <- x[valid]
  n <- length(theta)
  if (n == 0) stop("No valid data after removing NAs.")
  
  # Rank and compute theta star
  r_theta <- rank(theta, ties.method = "average")
  r_x <- rank(x, ties.method = "average")
  r_theta_star <- r_theta * 2 * pi / n
  
  # Precompute sine and cosine
  cos_theta_star <- cos(r_theta_star)
  sin_theta_star <- sin(r_theta_star)
  
  # Compute T_c and T_s
  T_c <- sum(r_x * cos_theta_star)
  T_s <- sum(r_x * sin_theta_star)
  
  # Calculate coefficient 'a'
  a <- ifelse(
    n %% 2 == 0,
    1 / (1 + 5 / tan(pi / n)^2 + 4 / tan(pi / n)^4),
    2 * sin(pi / n)^4 / (1 + cos(pi / n))^3
  )
  
  # Final D value (rank correlation, values between 0 and 1)
  D <- (a * (T_c^2 + T_s^2)) %>% sqrt()
  
  # Calculate U_n and p-value
  U_n <- 24 * (T_c^2 + T_s^2) / (n^2 * (n + 1))
  p.value <- 1 - pchisq(U_n, df = 2)
  
  # Return results as a list
  return(list(estimate = D, p.value = p.value, U_n = U_n))
}
```

To make the code cleaner, we use a helper function that, for correlations where $\phi$ is involved, calculates various correlations (Pearson's, Spearman's rank, Kendall's rank, as well as Johnson-Wehrly-Mardia, and Mardia's rank) for us:

```{r}

# Create correlation function with linear and circular methods
compute_correlations <- function(x, y) {
  
  # Removing NA pairs
  z <- cbind(x, y) %>% na.omit()
  x <- z[,1]
  y <- z[,2]
  
  ## x is a circular variable, y is a linear variable
  cor_mardia <- cor_mardia(x, y)
  
  tibble(
    ## Linear correlations
    # Pearson's
    pearson_cor = cor(x, y),
    pearson_pval = cor.test(x, y)$p.value,
    # Spearman's
    spearman_cor = cor(x, y, method = "spearman"),
    spearman_pval = cor.test(x, y, method = "spearman")$p.value,# Kendall's tau
    kendall_cor = cor(x, y, method = "kendall"),
    kendall_pval = cor.test(x, y, method = "kendall")$p.value,
    ## Circular correlations
    # Johnson-Wehrly-Mardia correlation (sinusoidal assumption)
    jwm_cor = tryCatch(
      circlin.cor(x, y)[1] %>% sqrt(),
      error = function(e)
        NA_real_
    ),
    jwm_pval = tryCatch(
      circlin.cor(x, y)[2],
      error = function(e)
        NA_real_
    ),
    # Mardia's rank correlation (no assumption)
    mard_cor = cor_mardia$estimate,
    mard_pval = cor_mardia$p.value
  )
}
```

We then use the functions to do the calculations, make a long dataframe, and save it as an `.RDS` file:

```{r}
#| label: extract-level2-estimates

# Add baseline bl_pa and bl_na to d_draws if not added yet
if(sum(c("bl_pa", "bl_na") %in% names(d_draws)) == FALSE)
  d_draws <- d_draws %>%
    full_join(d_bl,
              by = "id")

# Compute correlations for all combinations
cors_lin_angle <- d_draws %>%
  group_by(iteration) %>%
  summarise(
    crossing(
      linear_variable = c("mesor",
                          "logsigma",
                          "sigma",
                          "sigma2",
                          "bl_pa",
                          "bl_na",
                          "amp"),
      circular_variable = c("phi", "phi.begins6", "phi.begins12")
    ) %>%
      mutate(corr_data = map2(
        .x = syms(circular_variable),
        .y = syms(linear_variable),
        .f = ~ compute_correlations(eval_tidy(.x, data = cur_data()),
                                    eval_tidy(.y, data = cur_data()))
      )) %>%
      unnest(corr_data) %>%
      rename(par2 = circular_variable,
             par1 = linear_variable)
  ) %>%
  unnest(cols = last_col()) %>%
  pivot_longer(-1:-3,
               names_to = "cortypeXmeasure",
               values_to = "value")

beepr::beep(7)

cors_lin_lin <- d_draws %>%
  filter(iteration < 501) %>% 
  mutate(iteration = paste(item, iteration)) %>% 
    group_by(iteration) %>%
    summarise(
        pair_data = list(
            c("mesor", "amp", "logsigma", "sigma", "sigma2") %>% 
                combn(2) %>%
                t() %>%
                as.data.frame() %>%
                rename(par1 = V1, par2 = V2) %>% 
                tibble()
        )
    ) %>%
    unnest(pair_data) %>%
    mutate(
        x = map2(par1, iteration, ~ pull(filter(d_draws, iteration == .y), all_of(.x))),
        y = map2(par2, iteration, ~ pull(filter(d_draws, iteration == .y), all_of(.x))),
        pearson_cor = map2_dbl(x, y, ~ cor(.x, .y, use = "complete.obs")),
        pearson_pval = map2_dbl(x, y, ~ cor.test(.x, .y)$p.value),
        spearman_cor = map2_dbl(x, y, ~ cor(.x, .y, use = "complete.obs", method = "spearman")),
        spearman_pval = map2_dbl(x, y, ~ cor.test(.x, .y, method = "spearman")$p.value),
        kendall_cor = map2_dbl(x, y, ~ cor(.x, .y, use = "complete.obs", method = "kendall")),
        kendall_pval = map2_dbl(x, y, ~ cor.test(.x, .y, method = "kendall")$p.value)
    ) %>%
    select(-x, -y) %>%
    unnest(cols = last_col()) %>%
    pivot_longer(-1:-3,
                 names_to = "cortypeXmeasure",
                 values_to = "value")

ests_level2 <- #cors_lin_angle %>%
  rbind(cors_lin_lin) %>%
  group_by(cortypeXmeasure) %>%
  mutate(
    correlation_type = str_split(cortypeXmeasure, "_")[[1]][1],
    measure = str_split(cortypeXmeasure, "_")[[1]][2],
    .before = value
  ) %>%
  ungroup() %>%
  mutate(what = paste(par1, correlation_type, par2, sep = "_"),
         .after = correlation_type) %>%
  select(-cortypeXmeasure)

beepr::beep(9)

# ests_level2 %>%
#   cbind(item = item_, .) %>% 
# saveRDS(here("fits",
#              paste0("ests_level2_lin_",
#                     item_,
#                     "_random_var.rds")
#              )
#         )

```

# Results

Before making the plots, we first read the saved results:

```{r}

draws <-
  rbind(
    readRDS(here("fits",
                 "draws_pa_random_var.rds")),
    readRDS(here("fits",
                 "draws_fitbit_random_var.rds"))
    )

el1 <- e_level1 <-
  rbind(
    readRDS(here("fits",
                 "ests_level1_pa_random_var.rds")),
    readRDS(here("fits",
                 "ests_level1_fitbit_random_var.rds"))
    )

el1.5 <- e_level1.5 <- 
  rbind(
    readRDS(here("fits",
                 "ests_level1.5_pa_random_var.rds")),
    readRDS(here("fits",
                 "ests_level1.5_fitbit_random_var.rds"))
    )

el2 <- e_level2 <-
  rbind(
    readRDS(here("fits",
                 "ests_level2_pa_random_var.rds")),
    readRDS(here("fits",
                 "ests_level2_fitbit_random_var.rds"))
    )

```

## Level-1 plots

### Amplitude HDR

For the HDR plot we use `f_plot_hdr`, that plots the heatmap of the joint posterior distributions of $C_i$ and $S_i$, adds contour lines at 95, 95, 90, 85, and 80 %HDRs, and adds a line from the origin to the median of the joint distribution.

```{r}

f_plot_hdr <- function(df,
                       x_col = "co",
                       y_col = "si",
                       title = "HDR heatmap",
                       xlim = NULL,
                       ylim = NULL) {
  # Determine axis limits if not provided
  if (is.null(xlim) || is.null(ylim)) {
    co_range <- range(df[[x_col]], na.rm = TRUE)
    si_range <- range(df[[y_col]], na.rm = TRUE)
    max_range <- max(abs(c(co_range, si_range))) * 1.2  # Expand range by 20%
    xlim <- c(-max_range, max_range)
    ylim <- c(-max_range, max_range)
  }
  
  # Compute 2D density with extended limits
  density_est <- MASS::kde2d(df[[x_col]],
                             df[[y_col]],
                             n = 300,
                             lims = c(xlim[1], xlim[2], ylim[1], ylim[2]))  # Ensure density extends to plot edges
  
  # Convert density estimate to a dataframe
  density_df <- expand.grid(co = density_est$x, si = density_est$y) %>%
    mutate(density = as.vector(density_est$z)) %>%
    arrange(desc(density)) %>%
    mutate(cumulative_prob = cumsum(density) / sum(density) * 100)  # HDR
  
  # Compute median coordinates and distance from origin
  median_co <- median(df[[x_col]], na.rm = TRUE)
  median_si <- median(df[[y_col]], na.rm = TRUE)

  # Define color palette
  viridis_colors <- viridis(256, direction = -1)
  viridis_colors[1] <- "#FFFFFF"  # Make the lowest density white
  
  # Generate tick labels for axes (without decimals)
  x_ticks <- seq(xlim[1], xlim[2], by = 5) %>% round()
  y_ticks <- seq(ylim[1], ylim[2], by = 5) %>% round()
  x_tick_labels <- ifelse(x_ticks == 0, "", as.character(x_ticks))
  y_tick_labels <- ifelse(y_ticks == 0, "", as.character(y_ticks))
  
  # Create plot
  p <- ggplot(density_df, aes(x = co, y = si)) +
    # Heatmap with fine-grained HDR levels
    geom_tile(aes(fill = cumulative_prob)) +
    # Reverse viridis_colors and set legend breaks at 10% intervals
    scale_fill_gradientn(
      colors = rev(viridis_colors),
      breaks = seq(0, 100, by = 10),
      minor_breaks = seq(0, 100, by = 5),
      limits = c(0, 100),
      name = "HDR (%)"
    ) +
    # Add contour lines at specified HDR levels
    geom_contour(
      aes(z = cumulative_prob),
      breaks = 95, #c(95, 90, 85, 80),
      color = "gray30",
      size = 0.7
    ) +
    # Add axes at the center
    geom_hline(yintercept = 0,
               color = "black",
               alpha = 0.6,
               size = 0.4) +  # X-axis
    geom_vline(xintercept = 0,
               color = "black",
               alpha = 0.6,
               size = 0.4) +  # Y-axis
    # Add line from origin to median
    annotate(
      "segment",
      x = 0,
      xend = median_co,
      y = 0,
      yend = median_si,
      color = "red",
      size = 0.7
    ) +
    # Ensure square aspect ratio
    coord_fixed(ratio = 1,
                xlim = xlim,
                ylim = ylim) +
    # Theme and labels
    theme_few() +
    labs(x = TeX("$C_i$"), y = TeX("$S_i$"), title = title)
  
  return(p)
}
```

And make plots for Persons 76 and 179 of Dataset 1 (with the origin at 75.0%HDR and 98.9%HDR, respectively) of Dataset 1, and Persons 11 and 20 of Dataset 2 (with the origin at 83.9%HDR and 100%HDR, respectively).

```{r}

p_pa_76 <- draws %>%
  filter(item == "pa", id == 76) %>%
  f_plot_hdr(title = "Person D1-76")
p_pa_179 <- draws %>%
  filter(item == "pa", id == 179) %>%
  f_plot_hdr(title = "Person D1-179")

p_fitbit_11 <- draws %>%
  filter(item == "fitbit", id == 11) %>%
  f_plot_hdr(title = "Person D2-11")
p_fitbit_26 <- draws %>%
  filter(item == "fitbit", id == 26) %>%
  f_plot_hdr(title = "Person D2-26")


p_four <- (p_pa_76 / p_pa_179) | (p_fitbit_11 / p_fitbit_26)

p_four +
  plot_layout(guides = "collect") &
  theme(legend.position = 'right',
        legend.key.height = unit(1, "cm"),
        legend.title = element_text(size = 10))
# + plot_annotation("Posterior joint distribution of C_i and S_i")

ggsave("joint-posterior-hdr.pdf",
       width = 20,
       height = 16,
       units = "cm")
```

We print the person-specific amplitude and the corresponding HDR level of each individual, with percentage of individuals that fall within each HDR level:

```{r}

library(ggplot2)
library(dplyr)
library(viridis)

# Assuming el1_tmp is the dataframe
el1_tmp <- el1 %>%
  mutate(interval = cut(
    amp_hdr,
    breaks = c(0, 80, 85, 90, 95, 100),
    labels = c("0-80", "80-85", "85-90", "90-95", "95-100"),
    include.lowest = TRUE,
    ordered_result = TRUE
  ),
  item = case_when(
    item == "pa" ~ "Dataset 1",
    TRUE ~ "Dataset 2"
  ))

# Generate a complete set of intervals for each item to ensure missing intervals are represented
interval_template <- expand.grid(
  item = unique(el1_tmp$item),
  interval = levels(el1_tmp$interval),
  stringsAsFactors = FALSE
)

# Calculate percentages per interval per item
interval_counts <- el1_tmp %>%
  group_by(item) %>%
  mutate(total_per_item = n()) %>%
  group_by(item, interval) %>%
  summarise(n = n(),
            percent = round(n / first(total_per_item) * 100, 1),
            .groups = 'drop') %>%
  right_join(interval_template, by = c("item", "interval")) %>%
  mutate(n = replace_na(n, 0),
         percent = replace_na(percent, 0))

# Adjust interval y positions
interval_counts <- interval_counts %>%
  mutate(
    y_position = case_when(
      interval == "0-80" ~ 77.5,
      interval == "80-85" ~ 82.5,
      interval == "85-90" ~ 87.5,
      interval == "90-95" ~ 92.5,
      interval == "95-100" ~ 97.5
    )
  )

# Calculate max amp_median per item to position labels appropriately with better spacing
max_amp_median <- el1_tmp %>% 
  group_by(item) %>% 
  summarise(max_median = max(amp_median, na.rm = TRUE))

interval_counts <- interval_counts %>%
  left_join(max_amp_median, by = "item") %>%
  mutate(label_x_position = max_median * 1.015)  # Adjusted to a smaller shift

p_hist <- ggplot(el1_tmp, aes(x = amp_median)) +
  geom_histogram(alpha = 0.6,
                 bins = 40,
                 fill = "chartreuse3") +
  theme_minimal() +
  labs(x = "", y = "") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, NA)) +
  facet_wrap( ~ item,
              nrow = 1,
              scales = "free",
              strip.position = "top") +
  # Add line under dataset name in the facet title
  annotate("segment",
           x = Inf,
           xend = -Inf,
           y = Inf,
           yend = Inf,
           color = "black",
           lwd = 0.75) +
  theme(panel.spacing = unit(3, "lines"),
        strip.text = element_text(size = 12),
        legend.position = "none",
        axis.text = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        panel.background = element_blank(),
        plot.margin = margin(0,0,0,0)) 


# Main scatter plot with facet wrap by item and adjusted spacing
p_scatter <- ggplot(el1_tmp, aes(x = amp_median,
                     y = amp_hdr,
                     color = interval)) +
  geom_hline(
    yintercept = c(95, 90, 85, 80),
    linetype = "dotted",
    color = "red"
  ) +
  geom_point(alpha = 0.6) +
  scale_color_viridis_d(begin = 0.3, end = 0.95) +
  scale_fill_viridis_d(begin = 0.3, end = 0.95) +
  theme_minimal() +
  labs(x = "Person-specific Amplitude", y = TeX("$\\alpha$%HDR")) +
  geom_label(
    data = interval_counts,
    aes(
      x = label_x_position,
      y = y_position,
      label = paste0(percent, "%"),
      fill = interval
    ),
    alpha = 0.6,
    color = "black",
    hjust = 1.1,
    size = 3
    # label.size = 0  # Removes border for better readability
  ) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, NA)) +
  scale_y_continuous(breaks = seq(0, 100, 25) %>% c(80, 85, 90, 95)) +
  facet_wrap( ~ item,
              nrow = 1,
              scales = "free_x",
              strip.position = "top") +
  theme(panel.spacing = unit(3, "lines"),
        strip.text = element_blank(),#element_text(size = 12),
        legend.position = "none")


p_hist / p_scatter + patchwork::plot_layout(heights = c(0.5,2))


ggsave("l1-amp-hist-with-hdr.pdf",
       width = 20,
       height = 15,
       units = "cm")
```

For the figure in the conclusion subsection of level-1 estimation, we make a scatter plot of HRD levels based on 95%CI width:

```{r}


p_hist <- ggplot(el1_tmp, aes(x = phi_circ_ci_width*12/pi)) +
  geom_histogram(alpha = 0.6,
                 bins = 40,
                 fill = "orangered") +
  theme_minimal() +
  labs(x = "", y = "") +
  scale_x_continuous(expand = c(0, 0),
                     limits = c(0, 24),
                     breaks = (0:4) * 6) +
  facet_wrap( ~ item,
              nrow = 1,
              scales = "free",
              strip.position = "top") +
  # Add line under dataset name in the facet title
  annotate("segment",
           x = Inf,
           xend = -Inf,
           y = Inf,
           yend = Inf,
           color = "black",
           lwd = 0.75) +
  theme(panel.spacing = unit(3, "lines"),
        strip.text = element_text(size = 12),
        legend.position = "none",
        axis.text = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        panel.background = element_blank(),
        plot.margin = margin(0,0,0,0)) 

# Main scatter plot with facet wrap by item and adjusted spacing
p_scatter <- ggplot(el1_tmp, aes(x = phi_circ_ci_width*12/pi,
                     y = amp_hdr,
                     color = interval)) +
  geom_hline(
    yintercept = c(95, 90, 85, 80),
    linetype = "dotted",
    color = "red"
  ) +
  geom_point(alpha = 0.4) +
  scale_color_viridis_d(begin = 0.3, end = 0.95, option = "plasma") +
  scale_fill_viridis_d(begin = 0.3, end = 0.95, option = "plasma") +
  theme_minimal() +
  labs(x = "95%CI width of person-specific peak offset", y = TeX("$\\alpha$%HDR")) +
  geom_label(
    data = interval_counts,
    aes(
      x = 24,
      y = y_position,
      label = paste0(percent, "%"),
      fill = interval
    ),
    alpha = 0.6,
    color = "black",
    hjust = 1.1,
    size = 3
    # label.size = 0  # Removes border for better readability
  ) +
  scale_x_continuous(expand = c(0, 0),
                     limits = c(0, 24),
                     breaks = (0:4) * 6) +
  scale_y_continuous(breaks = seq(0, 100, 25) %>% c(80, 85, 90, 95)) +
  facet_wrap( ~ item,
              nrow = 1,
              scales = "free_x",
              strip.position = "top") +
  theme(panel.spacing = unit(3, "lines"),
        strip.text = element_blank(),#element_text(size = 12),
        legend.position = "none")

p_hist / p_scatter + patchwork::plot_layout(heights = c(0.5,2))

ggsave("l1-ciwidth-hist-with-hdr.pdf",
       width = 20,
       height = 15,
       units = "cm")
```

### Circular and linear histograms and estimates

We first make a function to make the plots:

```{r}

f_plot_circular_linear <- function(phi,
                                   title = "Circular and linear histograms",
                                   only_circle = FALSE,
                                   only_hist = FALSE,
                                   only_median = FALSE) {
  # Set up bins: one per minute in 24 hours (1440 bins)
  bins <- 24 * 60
  arc <- 2 * pi / bins
  breaks <- seq(0, 2 * pi, length.out = bins + 1)
  
  bins.count <- hist.default(phi, breaks = breaks, plot = FALSE, right = TRUE)$counts
  mids <- seq(arc / 2, 2 * pi - arc / 2, length.out = bins)
  
  inc <- 0.2 / max(bins.count)
  
  # Create background points (circular histogram) using a vectorised approach
  d <- do.call(rbind, lapply(seq_len(bins), function(i) {
    count <- bins.count[i]
    if (count == 0) return(NULL)
    j <- seq(0, count - 1)
    r <- 1 + j * inc
    data.frame(r = r, x = r * cos(mids[i]), y = r * sin(mids[i]))
  }))
  
  # Summarise estimates for both linear and circular methods
  d_summary <- data.frame(
    type    = c("linear", "circular"),
    mean    = c(mean(phi), (mean(circular(phi)) %% (2 * pi))),
    median  = c(median(phi), (median(circular(phi)) %% (2 * pi))),
    ci_2.5  = c(quantile(phi, 0.025), quantile(circular(phi), 0.025)),
    ci_97.5 = c(quantile(phi, 0.975), quantile(circular(phi), 0.975)),
    r       = c(0.85, 1)
  ) %>%
    mutate(ci_2.5 = if_else(ci_2.5 > ci_97.5, ci_2.5 - 2 * pi, ci_2.5))
  
  if(only_median == TRUE)
    d_summary$mean <- NA
  
  d_estimates <- d_summary %>%
    pivot_longer(cols = c(mean, median),
                 names_to = "measure",
                 values_to = "value") %>%
    mutate(x = cos(value) * 1.4,
           y = sin(value) * 1.4)
  
  # Circular plot
  p_circle <- ggplot() +
    # Cross lines
    geom_vline(xintercept = 0,
               linewidth = 0.3,
               color = "gray60") +
    geom_hline(yintercept = 0,
               linewidth = 0.3,
               color = "gray60") +
    # Outer circle
    geom_circle(aes(x0 = 0, y0 = 0, r = 1),
                color = "gray70",
                inherit.aes = FALSE) +
    # Background points
    geom_point(data = d, aes(x = x, y = y), size = 0.2, color = "azure4") +
    # Confidence bands as slices
    geom_polygon(
      data = d_summary %>%
        group_by(type) %>%
        summarise(
          x = c(0, cos(seq(ci_2.5, ci_97.5, length.out = 100)) * r),
          y = c(0, sin(seq(ci_2.5, ci_97.5, length.out = 100)) * r),
          .groups = "drop"
        ),
      aes(x = x, y = y, fill = type),
      alpha = 0.6
    ) +
    # Rays for mean/median
    geom_segment(
      data = d_summary %>%
        pivot_longer(cols = c(mean, median),
                    names_to = "measure",
                    values_to = "value"),
      aes(x = 0,
          y = 0,
          xend = cos(value) * r,
          yend = sin(value) * r,
          color = type,
          linetype = measure),
      linewidth = 1.2, alpha = 0.7
    ) +
    # Asterisk markers
    geom_point(
      data = d_summary %>%
        pivot_longer(cols = c(mean, median),
                     names_to = "measure",
                     values_to = "value"),
      aes(x = cos(value) * r,
          y = sin(value) * r,
          shape = measure,
          color = type),
      size = 3, stroke = 1
    ) +
    # Scales for fill, color, shape, and linetype
    scale_fill_manual(
      values = c("circular" = "#A5DB36FF", "linear" = "#9C179EFF"),
      name   = "Calculation method",
      breaks = c("linear", "circular"),
      labels = c("Linear", "Circular")
    ) +
    scale_color_manual(
      values = c("circular" = "#A5DB36FF", "linear" = "#9C179EFF"),
      name   = "Calculation method",
      breaks = c("linear", "circular"),
      labels = c("Linear", "Circular")
    ) +
    scale_shape_manual(
      values = c("mean" = 1, "median" = 8),
      name   = "Measure", labels = c("Mean", "Median")
    ) +
    scale_linetype_manual(
      values = c("mean" = "solid", "median" = "dashed"),
      name   = "Measure", labels = c("Mean", "Median")
    ) +
    guides(color = "none") +
    coord_fixed(xlim = c(-1.1, 1.1), ylim = c(-1.1, 1.1)) +
    theme_minimal() +
    theme(
      legend.position = "none",
      panel.grid = element_blank(),
      axis.text  = element_blank(),
      axis.ticks = element_blank(),
      axis.title = element_blank()
    ) +
    ggtitle(title)
  
  if(only_circle == TRUE)
    return(p_circle + theme(legend.position = "right"))
  
  # Histogram plot
  p_hist <- ggplot(data.frame(x = phi), aes(x = x)) +
    geom_histogram(aes(y = ..ncount..),
                   bins = 120,
                   alpha = 0.6,
                   fill = "azure4") +
    geom_segment(
      data = d_estimates,
      aes(x = value, xend = value, y = 0, yend = r^3 * 0.4,
          color = type, linetype = measure),
      linewidth = 1.5, alpha = 0.6, inherit.aes = FALSE, show.legend = FALSE
    ) +
    geom_point(
      data = d_estimates,
      aes(x = value, y = r^3 * 0.4, shape = measure, color = type),
      size = 3, stroke = 1, inherit.aes = FALSE
    ) +
    geom_rect(
      data = d_estimates %>%
        mutate(
          ci_2.5  = ci_2.5 %% (2 * pi),
          ci_97.5 = ci_97.5 %% (2 * pi),
          new_ci_2.5  = case_when(
            measure == "mean"   & ci_2.5 > ci_97.5 ~ 0,
            measure == "median" & ci_2.5 > ci_97.5 ~ ci_2.5,
            TRUE ~ ci_2.5
          ),
          new_ci_97.5 = case_when(
            measure == "mean"   & ci_2.5 > ci_97.5 ~ ci_97.5,
            measure == "median" & ci_2.5 > ci_97.5 ~ 2 * pi,
            TRUE ~ ci_97.5
          )
        ) %>%
        select(-ci_2.5, -ci_97.5) %>%
        rename(ci_2.5 = new_ci_2.5, ci_97.5 = new_ci_97.5) %>%
        select(type, ci_2.5, ci_97.5, r) %>%
        distinct(),
      aes(xmin = ci_2.5,
          xmax = ci_97.5,
          ymin = 0,
          ymax = r^3 * 0.4,
          fill = type),
      alpha = 0.6, inherit.aes = FALSE
    ) +
    theme_minimal() +
  scale_x_continuous(
    breaks = c(0, pi/2, pi, 3*pi/2, 2*pi),
    labels = c("0", expression(pi/2), expression(pi), expression(3*pi/2), expression(2*pi))
  ) +
    theme(
      panel.grid    = element_blank(),
      axis.text.y   = element_blank(),
      axis.ticks.y  = element_blank(),
      axis.title.y  = element_blank()
    ) +
    xlab("Position in cycle") +
    ylab(NULL) +
    guides(color = "none") +
    scale_fill_manual(
      values = c("circular" = "#A5DB36FF", "linear" = "#9C179EFF"),
      name   = "Calculation method",
      breaks = c("linear", "circular"),
      labels = c("Linear", "Circular")
    ) +
    scale_color_manual(
      values = c("circular" = "#A5DB36FF", "linear" = "#9C179EFF"),
      name   = "Calculation method",
      breaks = c("linear", "circular"),
      labels = c("Linear", "Circular")
    ) +
    scale_shape_manual(
      values = c("mean" = 1, "median" = 8),
      name   = "Measure", labels = c("Mean", "Median")
    ) +
    scale_linetype_manual(
      values = c("mean" = "solid", "median" = "dashed"),
      name   = "Measure", labels = c("Mean", "Median")
    ) +
    expand_limits(x = c(0, 2 * pi))
  
  if(only_hist == TRUE) return(p_hi)
  
  # Combine the circular and histogram plots using patchwork
  wrap_plots(p_circle, plot_spacer(), p_hist, ncol = 1, heights = c(1, 0.1, 0.5))
  
}
```

For print-friendly coloring, `"circular" = "#313695"` and `"linear" = "#F46D43"` (from `RColorBrewer::brewer.pal(11, "RdYlBu")`) can be used; however, they look really ugly on screen. So we'll use `"cornflowerblue"` and `"brown1"`, respectively.

And then make the plots for Persons D1-179 and D2-26:

```{r}

p_1 <- draws %>%
  filter(item == "pa", id == 179) %>%
  pull(phi) %>% 
  f_plot_cirular_linear(title = "Person D1-179")

p_2 <- draws %>%
  filter(item == "fitbit", id == 26) %>%
  pull(phi) %>% 
  f_plot_cirular_linear(title = "Person D2-26")

(p_1 | plot_spacer() | p_2) + plot_layout(widths = c(1, 0.1, 1), guides = "collect")

ggsave("distribution-circular-linear.pdf",
       width = 20,
       height = 15,
       units = "cm")
```

Now we make the plots for the distributions of estimated mean, median, and 95%CI width for datasets, for linear or circular methods. For that we first need to reshape the dataframe:

```{r}

el1_long <- el1 %>%
  pivot_longer(
    cols = matches("^phi"),
    names_to = "phi_type",
    values_to = "phi"
  ) %>%
  mutate(
    method = ifelse(str_detect(phi_type, "_circ"), 
                    "With circular approach",
                    "With linear approach"),
    starting_hour = case_when(
      str_detect(phi_type, "begins6") ~ 6,
      str_detect(phi_type, "begins12") ~ 12,
      TRUE ~ 0
    ),
    starting_hour_factor = case_when(
      str_detect(phi_type, "begins6") ~ "Starting at 6:00",
      str_detect(phi_type, "begins12") ~ "Starting at 12:00",
      TRUE ~ "Starting at 0:00"
    ) %>% 
      factor(levels = c("Starting at 0:00",
                        "Starting at 6:00",
                        "Starting at 12:00")),
    measure = case_when(
      str_detect(phi_type, "_mean$") ~ "Mean",
      str_detect(phi_type, "_median$") ~ "Median",
      str_detect(phi_type, "_ci_width$") ~ "95%CI width",
      TRUE ~ NA_character_
    ),
    # Streamlined psi calculation:
    psi = ifelse(
      measure == "95%CI width",
      phi * 12 / pi,
      ((phi * 12 / pi) + starting_hour) %% 24
    )
  ) %>%
  filter(!is.na(measure)) %>%
  select(
    item,
    id,
    method,
    starting_hour,
    starting_hour_factor,
    measure,
    phi,
    psi
  )
```

And we plot them:

```{r}

el1_long %>%
  mutate(
    measure = factor(measure, levels = c("Mean", "Median", "95%CI width")),
    item = case_when(item == "pa" ~ "Dataset 1", TRUE ~ "Dataset 2")
  ) %>%
  ggplot() +
  aes(x = psi, fill = method) +
  geom_histogram(
    aes(y = ..ncount..),
    bins = 24,
    position = "identity",
    breaks = seq(0, 24, by = 0.5),
    alpha = 0.6
  ) +
  facet_nested(starting_hour_factor ~ item + measure, scales = "fixed") +
  xlab("Hours") +
  scale_x_continuous(breaks = (0:4) * 6) +
  scale_fill_manual(values = c("#A5DB36FF", "#9C179EFF")) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    panel.spacing.x = unit(1.5, "lines"),
    ggh4x.facet.nestline = element_line(linewidth = 0.5),
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    legend.title = element_blank(),
    legend.text = element_text(size = 8),
    strip.text.x = element_text(size = 12),
    strip.text.y = element_text(size = 10),
    axis.title.x = element_text(size = 8),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.title.y = element_blank(),
    legend.key.height = unit(.5, "cm")
  )

ggsave("distribution-l1-phi.pdf",
       width = 25,
       height = 12.5,
       units = "cm")
```

### Peak offset estimates on 24-hour clock

We can also plot the point estimates and the associated 95%CI of individual peak offset $\psi_i$ over a 24-hour clock with the following code (which is not very succinct):

```{r}

# Function to convert radians to hours (0-24)
rad_to_hours <- function(rad) {
  ((rad * 24) / (2 * pi)) %% 24
}

# Function to handle circular confidence intervals
circular_ci_to_hours <- function(lower, upper) {
  lower_hours <- rad_to_hours(lower)
  upper_hours <- rad_to_hours(upper)
  
  # Handle cases where CI crosses midnight (0/24 hours)
  if (lower_hours > upper_hours) {
    # CI crosses midnight, need to adjust
    list(lower = lower_hours,
         upper = upper_hours,
         crosses_midnight = TRUE)
  } else {
    list(lower = lower_hours,
         upper = upper_hours,
         crosses_midnight = FALSE)
  }
}

# Prepare the data
plot_data <- ests_level1_ci_bounds %>%
  mutate(
    phi_hours = rad_to_hours(phi_circ_mean),
    phi_median_hours = rad_to_hours(phi_circ_median),
    ci_lower_hours = rad_to_hours(phi_circ_ci_2.5),
    ci_upper_hours = rad_to_hours(phi_circ_ci_97.5)
  ) %>%
  # Scale amplitude for radius separately for each dataset
  group_by(item) %>%
  mutate(radius = amp_median / max(amp_median, na.rm = TRUE) * 0.8) %>%
  ungroup()

# Alternative version with better handling of circular CIs that cross midnight
# This creates separate segments for CIs that wrap around midnight
plot_data_expanded <- plot_data %>%
  rowwise() %>%
  mutate(crosses_midnight = ci_lower_hours > ci_upper_hours) %>%
  ungroup()

# Create segments for CIs that don't cross midnight
normal_segments <- plot_data_expanded %>%
  filter(!crosses_midnight) %>%
  mutate(x_start = ci_lower_hours, x_end = ci_upper_hours)

# Create segments for CIs that cross midnight (split into two segments)
midnight_segments <- plot_data_expanded %>%
  filter(crosses_midnight) %>%
  {
    rbind(
      mutate(
        .,
        x_start = ci_lower_hours,
        x_end = 24,
        segment_part = "first"
      ),
      mutate(
        .,
        x_start = 0,
        x_end = ci_upper_hours,
        segment_part = "second"
      )
    )
  }

# Combine all segments
all_segments <- rbind(mutate(normal_segments, segment_part = "normal"),
                      midnight_segments)

# Enhanced plot with proper circular CI handling
p_enhanced <- ggplot(plot_data, aes(x = phi_hours, y = radius)) +
  # Add hour markers (clock face)
  geom_vline(
    xintercept = seq(0, 23, 1),
    color = "grey90",
    size = 0.3
  ) +
  geom_vline(
    xintercept = c(0, 6, 12, 18),
    color = "grey70",
    size = 0.5
  ) +
  
  # Add confidence interval arcs (with proper midnight handling)
  geom_segment(
    data = all_segments,
    aes(
      x = x_start,
      xend = x_end,
      y = radius,
      yend = radius,
      color = amp_hdr
    ),
    size = .5,
    alpha = 0.5
  ) +
  
  # Add point estimates
  geom_point(aes(x = phi_hours, y = radius, color = amp_hdr),
             size = .75,
             alpha = 0.8) +
  
  # Set up polar coordinates for clock appearance
  coord_polar(theta = "x",
              start = 0,
              direction = 1) +
  
  # Customize scales
  scale_x_continuous(
    limits = c(0, 24),
    breaks = seq(0, 24, 6),
    labels = c("0", "6", "12", "18", "24")
  ) +
  scale_y_continuous(
    limits = c(0, 1),
    breaks = seq(0.2, 1, 0.2),
    labels = function(x)
      round((x - 0.2) / 0.8 * max(plot_data$amp_median, na.rm = TRUE))
  ) +
  
  # Color scale for amplitude HDR
  scale_color_viridis_c(begin = 0.3, end = 0.95, option = "plasma") +
  
  # Facet by item
  facet_wrap( ~ item, ncol = 2) +

  # Theming
  theme_minimal() +
  theme(
    axis.title = element_blank(),
    axis.text.y = element_blank(),
    axis.text.x = element_text(size = 10, face = "bold"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    strip.text = element_text(size = 12, face = "bold"),
    legend.position = "none"
  )

print(p_enhanced)

ggsave("l1-polar-amp-ci-hdr.pdf",
       width = 20,
       height = 10,
       units = "cm")
```

## Level-2 plots

### HDR

```{r}

p_hdr_pa <- e_level1.5 %>% 
    filter(item == "pa") %>% 
    f_plot_hdr("co_median", "si_median",
               title = "Dataset 1")

p_hdr_fitbit <- e_level1.5 %>% 
    filter(item == "fitbit") %>% 
    f_plot_hdr("co_median", "si_median",
               title = "Dataset 2")

(p_hdr_pa | p_hdr_fitbit) +
  plot_layout(guides = "collect") &
  theme(legend.position = 'right',
        legend.key.height = unit(1, "cm"),
        legend.title = element_text(size = 10))
# + plot_annotation("Posterior joint distribution of C_i and S_i")

ggsave("joint-posterior-hdr-level2.pdf",
       width = 20,
       height = 16,
       units = "cm")

```

### Phase fixed effects

Making the plots:

```{r}
p_pa_lin <- el1.5 %>%
  filter(item == "pa") %>%
  pull(phi_median) %>%
  f_plot_circular_linear(only_circle = TRUE, only_median = TRUE) +
  ggtitle("Dataset 1", "Based on linear medians")

p_pa_circ <- el1.5 %>%
  filter(item == "pa") %>%
  pull(phi_circ_median) %>%
  f_plot_circular_linear(only_circle = TRUE, only_median = TRUE) +
  ggtitle("", "Based on circular medians")


p_fitbit_lin <- el1.5 %>%
  filter(item == "fitbit") %>%
  pull(phi_median) %>% 
  f_plot_circular_linear(only_circle = TRUE, only_median = TRUE) +
  ggtitle("Dataset 2", "Based on linear medians")

p_fitbit_circ <- el1.5 %>%
  filter(item == "fitbit") %>%
  pull(phi_circ_median) %>% 
  f_plot_circular_linear(only_circle = TRUE, only_median = TRUE) +
  ggtitle("", "Based on circular medians")


p_lin <- (p_pa_lin | plot_spacer() | p_fitbit_lin) +
  plot_layout(widths = c(1, 0.1, 1))
p_circ <- (p_pa_circ | plot_spacer() | p_fitbit_circ) +
  plot_layout(widths = c(1, 0.1, 1))

(p_lin/ p_circ) + plot_layout(guides = "collect")

ggsave("distribution-l2-phi.pdf",
       width = 20,
       height = 15,
       units = "cm")

```

Getting the point estimates and 95%CI width of the fixed effects:

```{r}
el1.5 %>%
  select(item, phi_median, phi_circ_median) %>% 
  pivot_longer(phi_median:phi_circ_median,
               names_to = "median_method",
               values_to = "phi") %>% 
  mutate(median_method = ifelse(
    str_detect(median_method, "_circ"),
    "With circular approach",
    "With linear approach")) %>% 
  group_by(item, median_method) %>%
  summarise(
    bind_cols(
      # Linear calculations
      compute_linear_stats(cur_data(), "phi"),
      # Circular calculations
      compute_circular_stats(cur_data(), "phi")
    )
    )
```

### Correlations

#### Illustrative plots

To make the plots demonstrating correlations using simulated data, we first make `f_plot_correlation_demonstration` function:

```{r}

f_plot_correlation_demonstration <- function(ed,
                                             main_title = "",
                                             # subtitle = "",
                                             only_scatter_plot = FALSE,
                                             only_mardia_plot = FALSE,
                                             point_size = 1){

# -------------- Main Plot --------------

# Linear: z ~ phi
fit_lin <- lm(z ~ phi, data = ed)
phi_grid <- seq(0, 2 * pi, length.out = 200)
df_lin   <- data.frame(phi = phi_grid)
df_lin$z <- predict(fit_lin, newdata = df_lin)

# Cosinor: z ~ cos(phi) + sin(phi)
fit_cos <- lm(z ~ cos(phi) + sin(phi), data = ed)
df_cos  <- data.frame(phi = phi_grid)
df_cos$z <- predict(fit_cos, newdata = df_cos)
# MESOR line
M <- coef(fit_cos)[1]

corr <- suppressWarnings(compute_correlations(ed$phi, ed$z))
lbl <- paste(
  sprintf("Pearson:     %.2f (p=%.2f)", corr$pearson_cor, corr$pearson_pval),
  sprintf("Spearman:  %.2f (p=%.2f)",
    corr$spearman_cor,
    corr$spearman_pval
  ),
  sprintf("JWM:           %.2f (p=%.2f)", corr$jwm_cor, corr$jwm_pval),
  sprintf("Mardia:        %.2f (p=%.2f)", corr$mard_cor, corr$mard_pval),
  sep = "\n"
)


p_main <- ggplot(ed, aes(x = phi, y = z)) +
  geom_point(color = "azure4", size = point_size) +
  labs(x = TeX("Circular variable \\phi"),
       y = "Linear variable Z",
       title = main_title) +
  geom_line(
    data = df_lin,
    aes(phi, z),
    color = "#9C179EFF",
    size = 1
  ) +
  geom_line(
    data = df_cos,
    aes(phi, z),
    color = "#54C568FF",
    size = 1
  ) +
  geom_hline(yintercept = M,
             color = "#A5DB36FF",
             linetype = "dashed") +
  # ylim(-26, +44) +
  annotate(
    "label",
    x = -.1,
    y = 1.1*max(ed$z),
    label = lbl,
    fill = "white",
    alpha = 0.3,
    hjust = 0,
    vjust = 1,
    size = 4
  ) +
  scale_x_continuous(
    breaks = c(0, pi/2, pi, 3*pi/2, 2*pi),
    labels = c("0", expression(pi/2), expression(pi), expression(3*pi/2), expression(2*pi))
  ) +
  theme_few() +
  theme(plot.title = element_text(size = 16))


if(only_scatter_plot == TRUE)
  return(p_main)

# -------------- Mardia Plot --------------

# Build histogram from rank-transform
r_phi  <- rank(ed$phi)
r_z  <- rank(ed$z)
phi_vec <- rep(2 * pi * r_phi / nrow(ed), times = r_z)

bins  <- min(length(r_phi), 24 * 60)
arc   <- 2 * pi / bins
brks  <- seq(0, 2 * pi, length.out = bins + 1)
h     <- hist.default(phi_vec,
                      breaks = brks,
                      plot = FALSE,
                      right = TRUE)
mids  <- seq(arc / 2, 2 * pi - arc / 2, length.out = bins)

bins_count <- h$counts
inc <- 0.5*point_size / max(bins_count, na.rm = TRUE)  # spacing outward

d <- do.call(rbind, lapply(seq_len(bins), function(i) {
  count <- bins_count[i]
  if (count == 0)
    return(NULL)
  j <- seq(0, count - 1)
  r <- 1 + j * inc
  data.frame(r = r,
             x = r * cos(mids[i]),
             y = r * sin(mids[i]))
}))

lim_mardia <- d %>% abs() %>% max()

T_c <- r_z * cos(r_phi * 2 * pi / nrow(ed))
T_s <- r_z * sin(r_phi * 2 * pi / nrow(ed))

resultant_l <- cor_mardia(ed$phi, ed$z)$estimate
resultant_angle <- atan2(sum(T_s), sum(T_c))

p_mardia <- ggplot() +
  geom_vline(xintercept = 0,
             linewidth = 0.3,
             color = "gray60") +
  geom_hline(yintercept = 0,
             linewidth = 0.3,
             color = "gray60") +
  geom_circle(aes(x0 = 0, y0 = 0, r = 1),
              color = "gray70",
              inherit.aes = FALSE) +
  geom_point(
    data = d,
    aes(x = x, y = y),
    size = 0.5*point_size,
    color = "azure4"
  ) +
  geom_segment(aes(
    x = 0,
    y = 0,
    xend = resultant_l * cos(resultant_angle),
    yend = resultant_l * sin(resultant_angle)
  ),
  arrow = arrow(type = "closed",
                        length = unit(point_size*10, "points")),
  color = "cornflowerblue"
  ) +
  coord_fixed(1,
              xlim = c(-lim_mardia, lim_mardia),
              ylim = c(-lim_mardia, lim_mardia)) +
  geom_circle(aes(x0 = 0, y0 = 0, r = resultant_l),
              linetype = "dashed",
              color = "cornflowerblue", inherit.aes = FALSE) +
  theme_minimal() +
  theme(
    legend.position = "none",
    panel.grid      = element_blank(),
    axis.text       = element_blank(),
    axis.ticks      = element_blank(),
    axis.title      = element_blank()
  )

if(only_mardia_plot == TRUE)
  return(p_mardia + ggtitle(main_title))

p_main + p_mardia +
  plot_layout(widths = c(2, 1))

}
```

We then simulate data for demonstration:

```{r}

set.seed(2025-03-07)
N <- 300
rn <- rnorm(N)
phi_sim <- runif(N, 0, 2*pi)
data.frame(phi = (phi_sim + 1.04*pi) %% (2*pi),
           z = 10*cos(phi_sim + pi/3*cos(phi_sim)) +
             1*rn) %>%
  f_plot_correlation_demonstration()

df_1 <- data.frame(phi = (phi_sim + 0.54*pi) %% (2*pi),
             z = cos(phi_sim + pi/3*cos(phi_sim)) +  0.1*rn)
df_2 <- data.frame(phi = (phi_sim + 0.04*pi) %% (2*pi),
             z = cos(phi_sim + pi/3*cos(phi_sim)) +  0.1*rn)
df_3 <- data.frame(phi = (phi_sim + 1.04*pi) %% (2*pi),
             z = cos(phi_sim + pi/3*cos(phi_sim)) +  0.1*rn)

df_1_noisy <- data.frame(phi = (phi_sim + 0.54*pi) %% (2*pi),
             z = cos(phi_sim + pi/3*cos(phi_sim)) +  1*rn)
df_2_noisy <- data.frame(phi = (phi_sim + 0.04*pi) %% (2*pi),
             z = cos(phi_sim + pi/3*cos(phi_sim)) +  1*rn)
df_3_noisy <- data.frame(phi = (phi_sim + 1.04*pi) %% (2*pi),
             z = cos(phi_sim + pi/3*cos(phi_sim)) +  1*rn)

```

And make the scatter plots:

```{r}

p_left <-
  f_plot_correlation_demonstration(df_1,
                                   "Raw simulated data",
                                   only_scatter_plot = TRUE) /
  f_plot_correlation_demonstration(df_2,
                                   expression(phi ~ "shifted by" ~ -pi/2),
                                   only_scatter_plot = TRUE) /
  f_plot_correlation_demonstration(df_3,
                                   expression(phi ~ "shifted by" ~ +pi/2),
                                   only_scatter_plot = TRUE)

p_right <-
  f_plot_correlation_demonstration(
    df_1_noisy,
    "Increased noise in Z",
    only_scatter_plot = TRUE) /
  f_plot_correlation_demonstration(
    df_2_noisy,
    expression("Increased noise in Z," ~ phi ~ "shifted by" ~ -pi/2),
    only_scatter_plot = TRUE) /
  f_plot_correlation_demonstration(
    df_3_noisy,
    expression("Increased noise in Z," ~ phi ~ "shifted by" ~ +pi/2),
    only_scatter_plot = TRUE)


ggsave("demonstration-correlation-comparison-scatter.pdf",
       p_left | p_right,
       height = 30,
       width = 45,
       units = "cm")
```

And the Mardia polar rank plots:

```{r}

p_left <-
  f_plot_correlation_demonstration(df_1,
                                   "Raw simulated data",
                                   only_mardia_plot = TRUE) /
  f_plot_correlation_demonstration(df_2,
                                   expression(phi ~ "shifted by" ~ -pi/2),
                                   only_mardia_plot = TRUE) /
  f_plot_correlation_demonstration(df_3,
                                   expression(phi ~ "shifted by" ~ +pi/2),
                                   only_mardia_plot = TRUE)

p_right <-
  f_plot_correlation_demonstration(
    df_1_noisy,
    "Increased noise in Z",
    only_mardia_plot = TRUE) /
  f_plot_correlation_demonstration(
    df_2_noisy,
    expression("Increased noise in Z," ~ phi ~ "shifted by" ~ -pi/2),
    only_mardia_plot = TRUE) /
  f_plot_correlation_demonstration(
    df_3_noisy,
    expression("Increased noise in Z," ~ phi ~ "shifted by" ~ +pi/2),
    only_mardia_plot = TRUE)


ggsave("demonstration-correlation-comparison-mardia.pdf",
       (p_left | plot_spacer() | p_right) + plot_layout(widths = c(1, 0.25, 1)),
       height = 25,
       width = 37.5,
       units = "cm")
```

#### Empirical results

```{r}

el2 %>%
  filter(grepl("phi", par2)) %>%
  # filter(!grepl("log", par1)) %>%
  pivot_wider(names_from = measure,
              values_from = value) %>%
  mutate(cor_abs = abs(cor)) %>%
  pivot_longer(cor:cor_abs,
               names_to = "measure",
               values_to = "value") %>%
  mutate(
    value = case_when(
      grepl("jwm", correlation_type) & measure == "cor" ~ sqrt(value),
      grepl("mard", correlation_type) & measure == "cor" ~ sqrt(value),
      TRUE ~ value
         ),
    variable = case_when(
      grepl("6", par2) ~ "Starting at 6:00",
      grepl("12", par2) ~ "Starting at 12:00",
      TRUE ~ "Starting at 0:00"
    ) %>% 
      factor(levels = c("Starting at 0:00",
                        "Starting at 6:00",
                        "Starting at 12:00")),,
    correlation_type =
      factor(
        correlation_type,
        levels = c("pearson", "spearman", "kendall", "jwm", "mard"),
        labels = c(
          "Pearson",
          "Spearman",
          "Kendall",
          "Johnson-Wehrly-Mardia",
          "Mardia"
        )
      )
  ) %>%
  group_by(item, correlation_type, par1, measure, variable) %>%
  filter(
    !grepl("Kendall", correlation_type, ignore.case = TRUE),
    measure == "cor",
    par1 %in% c("amp", "mesor", "logsigma")
  ) %>%
  mutate(mm = median (value),
         item = case_when(item == "pa" ~ "Dataset 1",
                          TRUE ~ "Dataset 2"),
         par1 = case_when(par1 == "amp" ~ "Amplitude",
                          par1 == "mesor" ~ "MESOR",
                          par1 == "logsigma" ~ "log(SD)",
                          TRUE ~ par1) %>% 
           factor(levels = c("MESOR", "Amplitude", "log(SD)"))) %>%
  mutate(measure = "") %>%
  ggplot(aes(x = value, fill = correlation_type)) +
  geom_vline(xintercept = 0.00, linetype = "dashed") +
  geom_histogram(
    aes(y = ..ncount..),
    bins = 100,
    color = NA,
    alpha = 0.6,
    position = "identity"
  ) +
  facet_nested(variable ~ item + par1, scales = "free_y", solo_line = TRUE) +
  # geom_vline(aes(xintercept = mm,
  #                color = correlation_type),
  #            linewidth = 0.5,
  #            show.legend = FALSE) +
  labs(x = "Value", y = "Frequency") +
  scale_fill_manual(
    values = c("#9C179EFF",
               "#C33D80FF",
               "#54C568FF",
               "#A5DB36FF"),
    name = "Correlation type"
  ) +
  scale_color_manual(values = c("#9C179EFF",
                                "#C33D80FF",
                                "#54C568FF",
                                "#A5DB36FF")) +
  
  theme_minimal() +
  # scale_y_continuous(transform = "sqrt") +
  theme(
    legend.position = "bottom",
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    legend.text = element_text(size = 7),
    panel.spacing.x = unit(1, "lines"),
    ggh4x.facet.nestline = element_line(linewidth = 0.5),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.title.y = element_blank(),
    strip.text = element_text(size = 10) # Increase panel label size
  ) +
  labs(fill = NULL)

ggsave("distribution-l2-phi-correlations.pdf",
       width = 25,
       height = 12.5,
       units = "cm")
```
